{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP-mixer _convert \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from erc.model import MLPMixer\n",
    "\n",
    "audio_output = torch.ones([3, 1024]) # Batch, Seq\n",
    "text_output = torch.ones([3, 768]) # Batch, Seq\n",
    "\n",
    "\n",
    "# Performs a batch matrix-matrix product of matrices stored in input and mat2.\n",
    "matmul_output = torch.bmm(audio_output.unsqueeze_(2), text_output.unsqueeze_(1))\n",
    "print(matmul_output.unsqueeze_(1).shape) # Batch, Color, Width, Hight\n",
    "\n",
    "\n",
    "model = MLPMixer(\n",
    "    image_size = (1024, 768),\n",
    "    channels = 1,\n",
    "    patch_size = 16,\n",
    "    dim = 512,\n",
    "    depth = 12,\n",
    "    num_classes = 7\n",
    ")\n",
    "pred = model(matmul_output) \n",
    "print(pred.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-Hub dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7])\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from glob import glob\n",
    "\n",
    "def get_hub_txt(self, txt: str, encoding: str = None)-> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if self.tokenizer:\n",
    "        result: dict = self.tokenizer(text=txt,\n",
    "                                    padding=\"max_length\",\n",
    "                                    truncation=\"only_first\",\n",
    "                                    max_length=self.max_length_txt,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_tensors=\"pt\")\n",
    "        input_ids = result[\"input_ids\"].squeeze()\n",
    "        mask = result[\"attention_mask\"].squeeze()\n",
    "        return input_ids, mask\n",
    "    else:\n",
    "        return txt, None\n",
    "    \n",
    "class AIHubDialog():\n",
    "    # PRETRAINED_DATA_PATH = '/home/hoesungryu/workspace/AI-Hub_emotion_dialog'\n",
    "    def __init__(self, PRETRAINED_DATA_PATH):\n",
    "        self.txt_folder = sorted(glob(os.path.join(PRETRAINED_DATA_PATH,'annotation')+'/*.csv'))\n",
    "        self.wav_folder = sorted(glob(os.path.join(PRETRAINED_DATA_PATH,'wav')+'/*.wav'))\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(glob(self.wav_folder)) == len(glob(self.txt_folder))\n",
    "        return len(glob(self.wav_folder)) \n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        data = {}\n",
    "        txt, _, emotion = pd.read_csv(self.txt_folder[idx]).iloc[0].values\n",
    "    \n",
    "        # Txt File\n",
    "        txt, txt_mask = self.get_hub_txt(txt_path=txt, encoding=self.TEXT_ENCODING)\n",
    "        data[\"txt\"] = txt\n",
    "        data[\"txt_mask\"] = txt_mask\n",
    "    \n",
    "        # emotion \n",
    "        data[\"emotion\"] = self.get_emo(emotion)\n",
    "\n",
    "        sampling_rate, wav, wav_mask = self.get_wav(wav_path=self.wav_folder[idx])\n",
    "        data[\"sampling_rate\"] = sampling_rate\n",
    "        data[\"wav\"] = wav\n",
    "        data[\"wav_mask\"] = wav_mask\n",
    "\n",
    "        return data\n",
    "    \n",
    "import random \n",
    "random.seed(42)\n",
    "\n",
    "@staticmethod\n",
    "def sampling_with_ratio(total_len : int, train_ratio = 0.8):\n",
    "    total_len = wav_folder \n",
    "    total_idx = [i for i in range(total_len)]\n",
    "    train_num = int(total_len * train_ratio)\n",
    "\n",
    "    train_idx = random.sample(total_idx, train_num)\n",
    "    valid_idx = list(set(total_idx) - set(train_idx))\n",
    "\n",
    "    return train_idx, valid_idx\n",
    "\n",
    "@staticmethod\n",
    "def get_multiple_elements_in_list(in_list, in_indices):\n",
    "    \"\"\"리스트에서 복수인덱스 값을 가져온다\"\"\"\n",
    "    return [in_list[i] for i in in_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Cross-entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Centroid ... \n",
    "from collections import defaultdict\n",
    "\n",
    "emotion_va_dic = defaultdict(dict)\n",
    "\n",
    "emotion_va_dic[emotion+'_arousal'] = arousal\n",
    "emotion_va_dic[emotion+'_valence'] = valence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emotion_idx in range(0,7):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import hydra\n",
    "\n",
    "import erc\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "\n",
    "train_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_train4\")\n",
    "valid_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_valid4\")\n",
    "\n",
    "\n",
    "train_dataloadaer = DataLoader(train_dataset, batch_size=2)\n",
    "sample = next(iter(train_dataloadaer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segment_id': ['Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  'Sess01_script01_F001',\n",
       "  'Sess01_script01_M001',\n",
       "  ...],\n",
       " 'sampling_rate': tensor([16000, 16000, 16000,  ..., 16000, 16000, 16000]),\n",
       " 'wav': tensor([[ 0.0039,  0.0019,  0.0029,  ...,  0.0013,  0.0021,  0.0027],\n",
       "         [ 0.0039,  0.0019,  0.0029,  ...,  0.0013,  0.0021,  0.0027],\n",
       "         [ 0.0039,  0.0019,  0.0029,  ...,  0.0013,  0.0021,  0.0027],\n",
       "         ...,\n",
       "         [-0.0648, -0.0621, -0.0434,  ...,  0.0001,  0.0001,  0.0001],\n",
       "         [-0.0648, -0.0621, -0.0434,  ...,  0.0001,  0.0001,  0.0001],\n",
       "         [-0.0648, -0.0621, -0.0434,  ...,  0.0001,  0.0001,  0.0001]]),\n",
       " 'wav_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'txt': tensor([[   2, 1406, 1535,  ...,    0,    0,    0],\n",
       "         [   2, 1406, 1535,  ...,    0,    0,    0],\n",
       "         [   2, 1406, 1535,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   2,   69,   19,  ...,    0,    0,    0],\n",
       "         [   2,   69,   19,  ...,    0,    0,    0],\n",
       "         [   2,   69,   19,  ...,    0,    0,    0]]),\n",
       " 'txt_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'emotion': tensor([0, 0, 0,  ..., 1, 1, 1]),\n",
       " 'valence': tensor([1.7000, 1.7000, 1.7000,  ..., 1.5000, 1.5000, 1.5000]),\n",
       " 'arousal': tensor([4.0000, 4.0000, 4.0000,  ..., 3.6000, 3.6000, 3.6000]),\n",
       " 'gender': tensor([0, 0, 0,  ..., 1, 1, 1])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[train_dataset['emotion'] ==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['emotion'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 898, 1: 365, 2: 1564, 3: 11586, 4: 2088, 5: 701, 6: 308})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class_count = Counter(train_dataset['emotion'].detach().cpu().numpy())\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([898, 365, 1564, 11586, 2088, 701, 308])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nSamples = class_count.values()\n",
    "nSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# nSamples = [887, 6130, 480, 317, 972, 101, 128]\n",
    "normedWeights = torch.FloatTensor([1 - (x / sum(nSamples)) for x in nSamples])\n",
    "# normedWeights = torch.FloatTensor(normedWeights)\n",
    "\n",
    "loss = nn.CrossEntropyLoss(normedWeights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import hydra\n",
    "import torch.nn as  nn \n",
    "import erc\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "\n",
    "train_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_train4\")\n",
    "valid_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_valid4\")\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_classweights(traindataset)-> torch.FloatTensor:\n",
    "    class_count = Counter(traindataset['emotion'].numpy())\n",
    "    nSamples = class_count.values()\n",
    "    normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
    "    return torch.FloatTensor(normedWeights)\n",
    "\n",
    "train_normedWeight = get_classweights(train_dataset)\n",
    "valid_normedWeight = get_classweights(valid_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9487, 0.9792, 0.9107, 0.3383, 0.8808, 0.9600, 0.9824])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss = nn.CrossEntropyLoss(train_normedWeight)\n",
    "train_normedWeight\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Ground Truth emotion in Multi-label-class\n",
    "\n",
    "```python \n",
    "\n",
    "logits = self.mlp_mixer(matmul_output) # (B, num_labels)\n",
    "\n",
    "# calcuate the loss fct\n",
    "cls_logits = logits[:, :-2]\n",
    "cls_labels = labels[\"emotion\"]\n",
    "if cls_labels.ndim == 1:\n",
    "    # Single label case\n",
    "    cls_loss = self.criterions[\"cls\"](cls_logits, cls_labels.long())\n",
    "elif cls_labels.ndim == 2:\n",
    "    # Multi label case\n",
    "    cls_loss = self.criterions[\"cls\"](cls_logits, cls_labels.float())\n",
    "    cls_labels = labels[\"emotion_deuce\"] # trainer 도 바꿔줘야됨 ... \n",
    "\n",
    "reg_logits = logits[:, -2:]\n",
    "reg_loss = self.criterions[\"reg\"](reg_logits, labels[\"regress\"].float())\n",
    "\n",
    "total_loss = cls_loss * self.cls_coef + reg_loss * self.reg_coef\n",
    "return {\n",
    "    \"loss\": total_loss,\n",
    "    \"cls_loss\": cls_loss.detach().cpu(),\n",
    "    \"reg_loss\": reg_loss.detach().cpu(),\n",
    "    \"emotion\": cls_labels.detach(),\n",
    "    \"regress\": labels[\"regress\"].detach(),\n",
    "    \"cls_pred\": cls_logits.detach(),\n",
    "    \"reg_pred\": reg_logits.detach(),\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4677</th>\n",
       "      <td>0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22188 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotion  valence  arousal\n",
       "0           0      1.7      4.0\n",
       "1           1      1.5      3.6\n",
       "2           2      1.3      4.3\n",
       "3           2      1.5      3.8\n",
       "4           1      1.6      3.7\n",
       "...       ...      ...      ...\n",
       "4673        3      3.0      3.0\n",
       "4674        0      2.7      3.5\n",
       "4675        3      2.9      3.5\n",
       "4676        3      2.3      3.5\n",
       "4677        0      3.1      3.8\n",
       "\n",
       "[22188 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'0_centroid': (2.7013428, 3.6017902),\n",
       "             '1_centroid': (1.8056872, 3.5810425),\n",
       "             '2_centroid': (1.8337096, 3.7831194),\n",
       "             '3_centroid': (2.989782, 3.1387265),\n",
       "             '4_centroid': (4.093699, 3.7420354),\n",
       "             '5_centroid': (1.9814088, 2.6549654),\n",
       "             '6_centroid': (2.2050884, 3.1471236)})"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get centroid \n",
    "import logging\n",
    "import hydra\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch.nn as  nn \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import erc\n",
    "\n",
    "train_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_train4\")\n",
    "valid_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_valid4\")\n",
    "\n",
    "\n",
    "def dataset2df(dataset, columns = ['emotion','valence','arousal']):\n",
    "    df = pd.DataFrame()\n",
    "    for column in columns:\n",
    "        df[column] = dataset[column]\n",
    "    return df\n",
    "\n",
    "train_df = dataset2df(train_dataset)\n",
    "valid_df = dataset2df(valid_dataset)\n",
    "\n",
    "total_df = pd.concat([train_df,valid_df], axis=0)\n",
    "display(total_df)\n",
    "# Get Centroid ... \n",
    "from collections import defaultdict\n",
    "\n",
    "emotion_va_dict = defaultdict(dict)\n",
    "\n",
    "for emotion_idx in range(0,7):\n",
    "    valence = total_df[total_df['emotion']==emotion_idx]['valence'].mean()\n",
    "    arousal = total_df[total_df['emotion']==emotion_idx]['arousal'].mean()\n",
    "    # print()\n",
    "    emotion_va_dict[f'{emotion_idx}_centroid'] = (valence, arousal)\n",
    "emotion_va_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9360 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotion  valence  arousal\n",
       "0           0      1.7      4.0\n",
       "1           1      1.5      3.6\n",
       "2           2      1.3      4.3\n",
       "3           2      1.5      3.8\n",
       "4           1      1.6      3.7\n",
       "...       ...      ...      ...\n",
       "1883        5      2.3      2.1\n",
       "1884        5      2.5      2.3\n",
       "1885        5      2.1      1.8\n",
       "1886        4      3.9      3.2\n",
       "1887        4      4.2      4.0\n",
       "\n",
       "[9360 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'0_centroid': (2.637565, 3.637669),\n",
       "             '1_centroid': (1.7393138, 3.6382587),\n",
       "             '2_centroid': (1.8028255, 3.8001664),\n",
       "             '3_centroid': (2.971001, 2.981144),\n",
       "             '4_centroid': (4.3137026, 3.8631923),\n",
       "             '5_centroid': (1.895839, 2.6122148),\n",
       "             '6_centroid': (2.205371, 3.1242967)})"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get centroid \n",
    "import logging\n",
    "import hydra\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch.nn as  nn \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import erc\n",
    "\n",
    "train_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19_train4\")\n",
    "valid_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19_valid4\")\n",
    "\n",
    "\n",
    "def dataset2df(dataset, columns = ['emotion','valence','arousal']):\n",
    "    df = pd.DataFrame()\n",
    "    for column in columns:\n",
    "        df[column] = dataset[column]\n",
    "    return df\n",
    "\n",
    "train_df = dataset2df(train_dataset)\n",
    "valid_df = dataset2df(valid_dataset)\n",
    "\n",
    "total_df = pd.concat([train_df,valid_df], axis=0)\n",
    "display(total_df)\n",
    "# Get Centroid ... \n",
    "from collections import defaultdict\n",
    "\n",
    "emotion_va_dict = defaultdict(dict)\n",
    "\n",
    "for emotion_idx in range(0,7):\n",
    "    valence = total_df[total_df['emotion']==emotion_idx]['valence'].mean()\n",
    "    arousal = total_df[total_df['emotion']==emotion_idx]['arousal'].mean()\n",
    "    # print()\n",
    "    emotion_va_dict[f'{emotion_idx}_centroid'] = (valence, arousal)\n",
    "emotion_va_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788</th>\n",
       "      <td>3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789</th>\n",
       "      <td>0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12828 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotion  valence  arousal\n",
       "0           3      3.4      2.9\n",
       "1           4      4.1      3.4\n",
       "2           3      3.1      2.8\n",
       "3           3      3.1      3.1\n",
       "4           3      3.4      3.4\n",
       "...       ...      ...      ...\n",
       "2785        3      3.0      3.0\n",
       "2786        0      2.7      3.5\n",
       "2787        3      2.9      3.5\n",
       "2788        3      2.3      3.5\n",
       "2789        0      3.1      3.8\n",
       "\n",
       "[12828 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'0_centroid': (3.0942307, 3.380769),\n",
       "             '1_centroid': (2.3906977, 3.0767443),\n",
       "             '2_centroid': (2.2208333, 3.5694444),\n",
       "             '3_centroid': (2.9960432, 3.1912591),\n",
       "             '4_centroid': (3.838546, 3.6015217),\n",
       "             '5_centroid': (2.5082645, 2.918182),\n",
       "             '6_centroid': (2.2032788, 3.2934425)})"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get centroid \n",
    "import logging\n",
    "import hydra\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch.nn as  nn \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import erc\n",
    "\n",
    "train_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy20_train4\")\n",
    "valid_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy20_valid4\")\n",
    "\n",
    "\n",
    "def dataset2df(dataset, columns = ['emotion','valence','arousal']):\n",
    "    df = pd.DataFrame()\n",
    "    for column in columns:\n",
    "        df[column] = dataset[column]\n",
    "    return df\n",
    "\n",
    "train_df = dataset2df(train_dataset)\n",
    "valid_df = dataset2df(valid_dataset)\n",
    "\n",
    "total_df = pd.concat([train_df,valid_df], axis=0)\n",
    "display(total_df)\n",
    "# Get Centroid ... \n",
    "from collections import defaultdict\n",
    "\n",
    "emotion_va_dict = defaultdict(dict)\n",
    "\n",
    "for emotion_idx in range(0,7):\n",
    "    valence = total_df[total_df['emotion']==emotion_idx]['valence'].mean()\n",
    "    arousal = total_df[total_df['emotion']==emotion_idx]['arousal'].mean()\n",
    "    # print()\n",
    "    emotion_va_dict[f'{emotion_idx}_centroid'] = (valence, arousal)\n",
    "emotion_va_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation \n",
    "\n",
    "import erc\n",
    "\n",
    "validation_fold: int = 4\n",
    "PRETRAINED_DATA_PATH: str = \"./aihub\"\n",
    "mode: str = \"train\"\n",
    "wav_processor: str = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
    "sampling_rate: int = 16_000\n",
    "wav_max_length: int = 112_000 # 16_000 * 7, 7secs duration\n",
    "txt_processor: str = \"klue/bert-base\"\n",
    "txt_max_length: int = 64\n",
    "multilabel: bool = True\n",
    "load_from_cache_file: bool = True\n",
    "num_proc: int = 8\n",
    "batched: bool = True\n",
    "batch_size: int = 1000 # Not a torch batch_size\n",
    "writer_batch_size: int = 1000\n",
    "num_data: int = None\n",
    "preprocess: bool = True\n",
    "\n",
    "\n",
    "ds_kwargs = dict(\n",
    "    # Note for hard-coded kwargs\n",
    "    # generate_csv=False,\n",
    "    return_bio=False,\n",
    "    tokenizer_name=None,\n",
    "    max_length_wav=wav_max_length,\n",
    "    max_length_txt=txt_max_length,\n",
    "    multilabel=multilabel,\n",
    "    validation_fold=validation_fold,\n",
    "    mode=mode,\n",
    "    num_data=num_data,\n",
    "    # PRETRAINED_DATA_PATH=PRETRAINED_DATA_PATH,\n",
    ")\n",
    "\n",
    "ds = erc.datasets.KEMDDataset(**ds_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from erc.constants import emotion_va_19_dict, emotion_va_20_dict, emotion_va_19_20_dict\n",
    "\n",
    "# paths from HF_KEMD __init__ paths\n",
    "\n",
    "if paths =='kemdy19':\n",
    "    emotion_va_dict = emotion_va_19_dict\n",
    "elif paths == 'kemdy20':\n",
    "    emotion_va_dict = emotion_va_19_dict\n",
    "elif paths == 'kemdy19-kemdy20':\n",
    "    emotion_va_dict = emotion_va_19_20_dict\n",
    "\n",
    "def find_deuce_label(regress : torch.tensor, deuce_mask : torch.tensor) -> torch.tensor:\n",
    "    '''\n",
    "    Summary:\n",
    "        동률인 것 중 거리가 가장 작은 것을 사용하여 라벨을 확정한다. \n",
    "    Input\n",
    "       regress = torch.tensor([valence, arousal])\n",
    "       deuce_maks = tensor([False,  True, False,  True, False, False, False])\n",
    "    Output\n",
    "        return minmum index in deuce_mask\n",
    "    '''\n",
    "    \n",
    "    deuce_mask = deuce_mask.nonzero().squeeze().numpy()\n",
    "    total_dist = []\n",
    "    for index in deuce_mask:\n",
    "        item = emotion_va_dict[f'{index}_centroid']\n",
    "        tmp_va_tensor = torch.tensor(item)\n",
    "        \n",
    "        tmp_dist = torch.norm(regress-tmp_va_tensor, 2, dim=0)\n",
    "        total_dist.append(tmp_dist)\n",
    "\n",
    "    total_dist = torch.stack(total_dist, -1) # reshape -1 \n",
    "    return torch.tensor(deuce_mask[total_dist.argmin()]) # select minumum value index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max 두개 이상 \n",
      "tensor([0.0000, 0.4000, 0.2000, 0.4000, 0.0000, 0.0000, 0.0000],\n",
      "       dtype=torch.float64)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for batch in ds:\n",
    "    tmp_emotion = torch.tensor(batch['emotion'].astype(float))\n",
    "    deuce_mask = (tmp_emotion == tmp_emotion.max())\n",
    "    if len(tmp_emotion[deuce_mask]) > 1:\n",
    "        regress = torch.stack([batch['valence'], batch[\"arousal\"]])\n",
    "        gt_emotion = find_deuce_label(regress, deuce_mask)\n",
    "\n",
    "    else:\n",
    "\n",
    "        gt_emotion = tmp_emotion.argmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False,  True, False, False, False])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deuce_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(7,).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.0000, 0.4000, 0.2000, 0.4000, 0.0000, 0.0000, 0.0000]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "deuce_mask == True\n",
    "for index in deuce_mask.nonzero().squeeze().numpy():\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False,  True, False, False, False])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function defaultdict.values>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_va_dict = {\n",
    "    '0_centroid': (2.7013428, 3.6017902),\n",
    "    '1_centroid': (1.8056872, 3.5810425),\n",
    "    '2_centroid': (1.8337096, 3.7831194),\n",
    "    '3_centroid': (2.989782, 3.1387265),\n",
    "    '4_centroid': (4.093699, 3.7420354),\n",
    "    '5_centroid': (1.9814088, 2.6549654),\n",
    "    '6_centroid': (2.2050884, 3.1471236)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[230], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m emotion_va_dict[\u001b[39m1\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "emotion_va_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:\n",
      " tensor([[ 1.5095, -0.1732,  0.7325,  0.7860, -0.3720,  1.2313,  0.4379,  0.6280,\n",
      "          0.9031],\n",
      "        [ 0.9350,  0.6143,  1.5245,  0.6916,  1.9839, -1.8993,  0.2561, -0.2878,\n",
      "          0.4371],\n",
      "        [ 1.0684,  0.5397, -0.1996,  1.1719,  1.3606,  0.0775,  0.5241, -0.8296,\n",
      "         -1.0481]])\n",
      "gt:\n",
      " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cls_logits   = torch.randn([3, 9]) # Batch, Seq\n",
    "cls_labels  = torch.ones([3, 9]) # Batch, Seq\n",
    "\n",
    "emotion = torch.ones(1)\n",
    "gt_arousal = torch.tensor([[3.],[2.0],[3.1]])# labels[\"arousal\"]\n",
    "gt_valence = torch.tensor([[3.],[2.0],[3.1]])# labels[\"valence\"]\n",
    "\n",
    "print('pred:\\n',cls_logits)\n",
    "print('gt:\\n',cls_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "elif cls_labels.ndim == 2:\n",
    "    cls_loss = self.criterions[\"cls\"](cls_logits, cls_labels.float())\n",
    "\n",
    "    print(cls_labels.max(dim=1).values)\n",
    "    if len(cls_labels.max(dim=1).values) > 1:\n",
    "        # Compare distance with Centroid    \n",
    "    else:\n",
    "        # \n",
    "        cls_labels = cls_labels.argmax(dim=1)\n",
    "\n",
    "    \n",
    "print(len(cls_labels.max(dim=1).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "cls_labels  = torch.ones([3, 9]) # Batch, Seq\n",
    "if len(cls_labels.max(dim=1).values) > 1:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 5, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_labels  = torch.randn([3, 9]) # Batch, Seq\n",
    "cls_labels.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:erc.datasets:Instantiate train Dataset\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max 두개 이상 \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.4000, 0.2000, 0.4000, 0.0000, 0.0000, 0.0000],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2000, 3.1000])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.9000, 0.1000, 0.0000, 0.0000],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7000, 4.0000])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_emotion = torch.tensor([3,1,3,1,0,2,0])\n",
    "emotion_max = tmp_emotion.max()\n",
    "\n",
    "# max 값이 두개 이상이면 그때 거리로 구한다. \n",
    "if len(tmp_emotion[tmp_emotion == tmp_emotion.max()]) > 1:\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_kwargs = dict(\n",
    "    # Note for hard-coded kwargs\n",
    "    # generate_csv=False,\n",
    "    return_bio=False,\n",
    "    tokenizer_name=None,\n",
    "    max_length_wav=wav_max_length,\n",
    "    max_length_txt=txt_max_length,\n",
    "    multilabel=multilabel,\n",
    "    validation_fold=validation_fold,\n",
    "    mode=\"valid\",\n",
    "    num_data=num_data,\n",
    "    # PRETRAINED_DATA_PATH=PRETRAINED_DATA_PATH,\n",
    ")\n",
    "\n",
    "ds = erc.datasets.KEMDDataset(**ds_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in ds:\n",
    "    tmp_emotion = torch.tensor(batch['emotion'].astype(float))\n",
    "    if len(tmp_emotion.unique()) > 1:\n",
    "        continue\n",
    "        gt_emotion = tmp_emotion.argmax()\n",
    "    else:\n",
    "        regress = torch.stack([batch['valence'], batch[\"arousal\"]])\n",
    "        gt_emotion = find_duece_label(regress)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gt_emotion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gt_emotion\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gt_emotion' is not defined"
     ]
    }
   ],
   "source": [
    "gt_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "if len(tmp_emotion.unique()) > 1:\n",
    "    gt_emotion = tmp_emotion.argmax()\n",
    "else:\n",
    "    gt_emotion = find_duece_label(tmp_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_emotion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.1000, 0.9000], dtype=torch.float64)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(tmp_emotion.unique())\n",
    "print(len(tmp_emotion.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_emotion.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_labels  = torch.ones(7,) # Batch, Seq\n",
    "cls_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "print(len(cls_labels.max(dim=1).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7000, 4.0000])\n"
     ]
    }
   ],
   "source": [
    "for batch in ds:\n",
    "    regress = torch.stack([batch['valence'], batch[\"arousal\"]])\n",
    "    print(regress)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0776, 0.4321, 0.2548, 1.5509, 2.4076, 1.3742, 0.9912])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0776, 0.4321, 0.2548, 1.5509, 2.4076, 1.3742, 0.9912])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dist = torch.stack(total_dist, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0776, 0.4321, 0.2548, 1.5509, 2.4076, 1.3742, 0.9912])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outx = []\n",
    "for i in range(5):\n",
    "    tmp = net(x) # this will return a 10x10 tensor\n",
    "    outx.append(tmp)\n",
    "\n",
    "outx = torch.stack(outx, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0776)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress = torch.tensor([[ds[0]['valence'], ds[0]['arousal']],\n",
    "                       [ds[1]['valence'], ds[1]['arousal']],\n",
    "                       [ds[2]['valence'], ds[2]['arousal']],\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7000, 4.0000],\n",
       "        [1.5000, 3.6000],\n",
       "        [1.3000, 4.3000]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(regress-regress, 2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9814, 2.6550],\n",
       "        [1.9814, 2.6550],\n",
       "        [1.9814, 2.6550]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_va_tensor = torch.tensor((1.9814088, 2.6549654))\n",
    "batch_size = 3\n",
    "tmp_va_tensor.repeat(batch_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, *_ = regress.shape\n",
    "\n",
    "total_dist = []\n",
    "total_dist = torch.tensor()\n",
    "for key, item in emotion_va_dict.items():\n",
    "    tmp_va_tensor = torch.tensor(item).repeat(batch_size,1)\n",
    "    \n",
    "    tmp_dist = torch.norm(regress-tmp_va_tensor, 2, dim=1)\n",
    "    total_dist.append(tmp_dist)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1.0776, 1.2013, 1.5656]),\n",
       " tensor([0.4321, 0.3063, 0.8790]),\n",
       " tensor([0.2548, 0.3807, 0.7430]),\n",
       " tensor([1.5509, 1.5596, 2.0503]),\n",
       " tensor([2.4076, 2.5976, 2.8489]),\n",
       " tensor([1.3742, 1.0606, 1.7806]),\n",
       " tensor([0.9912, 0.8380, 1.4657])]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "norm() received an invalid combination of arguments - got (Tensor, Tensor, keepdim=bool, dim=list), but expected one of:\n * (Tensor input, Number p)\n * (Tensor input, Number p, *, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: keepdim, dim\n * (Tensor input, Number p, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, Number p, tuple of ints dim, bool keepdim, *, Tensor out)\n * (Tensor input, Number p, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, Number p, tuple of names dim, bool keepdim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mnorm(regress,tmp_va_tensor)\n",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/torch/functional.py:1485\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(p, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1484\u001b[0m         _dim \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)]  \u001b[39m# noqa: C416 TODO: rewrite as list(range(m))\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m         \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mnorm(\u001b[39minput\u001b[39;49m, p, dim\u001b[39m=\u001b[39;49m_dim, keepdim\u001b[39m=\u001b[39;49mkeepdim)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[39m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m \u001b[39m# remove the overloads where dim is an int and replace with BraodcastingList1\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m \u001b[39m# and remove next four lines, replace _dim with dim\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \u001b[39mif\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: norm() received an invalid combination of arguments - got (Tensor, Tensor, keepdim=bool, dim=list), but expected one of:\n * (Tensor input, Number p)\n * (Tensor input, Number p, *, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: keepdim, dim\n * (Tensor input, Number p, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, Number p, tuple of ints dim, bool keepdim, *, Tensor out)\n * (Tensor input, Number p, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, Number p, tuple of names dim, bool keepdim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "torch.norm(regress, tmp_va_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m labels \u001b[39m=\u001b[39m {}\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m ds:\n\u001b[0;32m----> 3\u001b[0m     torch\u001b[39m.\u001b[39;49mstack([batch[\u001b[39m\"\u001b[39;49m\u001b[39mvalence\u001b[39;49m\u001b[39m\"\u001b[39;49m], batch[\u001b[39m\"\u001b[39;49m\u001b[39marousal\u001b[39;49m\u001b[39m\"\u001b[39;49m]], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m      4\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "labels = {}\n",
    "for batch in ds:\n",
    "    torch.stack([batch[\"valence\"], batch[\"arousal\"]], dim=1).float()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weight with Torchlightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ERCModule.__init__() missing 3 required positional arguments: 'train_loader', 'valid_loader', and 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39merc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m MLP_Mixer\n\u001b[1;32m      4\u001b[0m WEIGHTS_PATH \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/hoesungryu/etri-erc/weights_AI_HUB/26908-valid_acc0.994.ckpt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m ERCModule(MLP_Mixer)\u001b[39m.\u001b[39mload_from_checkpoint(WEIGHTS_PATH)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mlearning_rate)\n",
      "\u001b[0;31mTypeError\u001b[0m: ERCModule.__init__() missing 3 required positional arguments: 'train_loader', 'valid_loader', and 'optimizer'"
     ]
    }
   ],
   "source": [
    "from erc.trainer import ERCModule\n",
    "from erc.model import MLP_Mixer\n",
    "\n",
    "WEIGHTS_PATH = '/home/hoesungryu/etri-erc/weights_AI_HUB/26908-valid_acc0.994.ckpt'\n",
    "\n",
    "erc.trainer.train(config)\n",
    "model = ERCModule(MLP_Mixer).load_from_checkpoint(WEIGHTS_PATH)\n",
    "print(model.learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change loss function\n",
    "\n",
    "```python\n",
    "erc.optims.multi_class_focal_loss import FocalLoss\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
