{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP-mixer _convert \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from erc.model import MLPMixer\n",
    "\n",
    "audio_output = torch.ones([3, 1024]) # Batch, Seq\n",
    "text_output = torch.ones([3, 768]) # Batch, Seq\n",
    "\n",
    "\n",
    "# Performs a batch matrix-matrix product of matrices stored in input and mat2.\n",
    "matmul_output = torch.bmm(audio_output.unsqueeze_(2), text_output.unsqueeze_(1))\n",
    "print(matmul_output.unsqueeze_(1).shape) # Batch, Color, Width, Hight\n",
    "\n",
    "\n",
    "model = MLPMixer(\n",
    "    image_size = (1024, 768),\n",
    "    channels = 1,\n",
    "    patch_size = 16,\n",
    "    dim = 512,\n",
    "    depth = 12,\n",
    "    num_classes = 7\n",
    ")\n",
    "pred = model(matmul_output) \n",
    "print(pred.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-Hub dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7])\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from glob import glob\n",
    "\n",
    "def get_hub_txt(self, txt: str, encoding: str = None)-> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if self.tokenizer:\n",
    "        result: dict = self.tokenizer(text=txt,\n",
    "                                    padding=\"max_length\",\n",
    "                                    truncation=\"only_first\",\n",
    "                                    max_length=self.max_length_txt,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_tensors=\"pt\")\n",
    "        input_ids = result[\"input_ids\"].squeeze()\n",
    "        mask = result[\"attention_mask\"].squeeze()\n",
    "        return input_ids, mask\n",
    "    else:\n",
    "        return txt, None\n",
    "    \n",
    "class AIHubDialog():\n",
    "    # PRETRAINED_DATA_PATH = '/home/hoesungryu/workspace/AI-Hub_emotion_dialog'\n",
    "    def __init__(self, PRETRAINED_DATA_PATH):\n",
    "        self.txt_folder = sorted(glob(os.path.join(PRETRAINED_DATA_PATH,'annotation')+'/*.csv'))\n",
    "        self.wav_folder = sorted(glob(os.path.join(PRETRAINED_DATA_PATH,'wav')+'/*.wav'))\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(glob(self.wav_folder)) == len(glob(self.txt_folder))\n",
    "        return len(glob(self.wav_folder)) \n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        data = {}\n",
    "        txt, _, emotion = pd.read_csv(self.txt_folder[idx]).iloc[0].values\n",
    "    \n",
    "        # Txt File\n",
    "        txt, txt_mask = self.get_hub_txt(txt_path=txt, encoding=self.TEXT_ENCODING)\n",
    "        data[\"txt\"] = txt\n",
    "        data[\"txt_mask\"] = txt_mask\n",
    "    \n",
    "        # emotion \n",
    "        data[\"emotion\"] = self.get_emo(emotion)\n",
    "\n",
    "        sampling_rate, wav, wav_mask = self.get_wav(wav_path=self.wav_folder[idx])\n",
    "        data[\"sampling_rate\"] = sampling_rate\n",
    "        data[\"wav\"] = wav\n",
    "        data[\"wav_mask\"] = wav_mask\n",
    "\n",
    "        return data\n",
    "    \n",
    "import random \n",
    "random.seed(42)\n",
    "\n",
    "@staticmethod\n",
    "def sampling_with_ratio(total_len : int, train_ratio = 0.8):\n",
    "    total_len = wav_folder \n",
    "    total_idx = [i for i in range(total_len)]\n",
    "    train_num = int(total_len * train_ratio)\n",
    "\n",
    "    train_idx = random.sample(total_idx, train_num)\n",
    "    valid_idx = list(set(total_idx) - set(train_idx))\n",
    "\n",
    "    return train_idx, valid_idx\n",
    "\n",
    "@staticmethod\n",
    "def get_multiple_elements_in_list(in_list, in_indices):\n",
    "    \"\"\"리스트에서 복수인덱스 값을 가져온다\"\"\"\n",
    "    return [in_list[i] for i in in_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Cross-entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import hydra\n",
    "\n",
    "import erc\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "\n",
    "train_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_train4\")\n",
    "valid_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_valid4\")\n",
    "\n",
    "\n",
    "train_dataloadaer = DataLoader(train_dataset, batch_size=2)\n",
    "sample = next(iter(train_dataloadaer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['emotion'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 898, 1: 365, 2: 1564, 3: 11586, 4: 2088, 5: 701, 6: 308})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class_count = Counter(train_dataset['emotion'].detach().cpu().numpy())\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([898, 365, 1564, 11586, 2088, 701, 308])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nSamples = class_count.values()\n",
    "nSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# nSamples = [887, 6130, 480, 317, 972, 101, 128]\n",
    "normedWeights = torch.FloatTensor([1 - (x / sum(nSamples)) for x in nSamples])\n",
    "# normedWeights = torch.FloatTensor(normedWeights)\n",
    "\n",
    "loss = nn.CrossEntropyLoss(normedWeights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import hydra\n",
    "import torch.nn as  nn \n",
    "import erc\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "\n",
    "train_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_train4\")\n",
    "valid_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_valid4\")\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_classweights(traindataset)-> torch.FloatTensor:\n",
    "    class_count = Counter(traindataset['emotion'].numpy())\n",
    "    nSamples = class_count.values()\n",
    "    normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
    "    return torch.FloatTensor(normedWeights)\n",
    "\n",
    "train_normedWeight = get_classweights(train_dataset)\n",
    "valid_normedWeight = get_classweights(valid_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9487, 0.9792, 0.9107, 0.3383, 0.8808, 0.9600, 0.9824])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss = nn.CrossEntropyLoss(train_normedWeight)\n",
    "train_normedWeight\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Ground Truth emotion in Multi-label-class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import erc\n",
    "\n",
    "validation_fold: int = 4\n",
    "PRETRAINED_DATA_PATH: str = \"./aihub\"\n",
    "mode: str = \"train\"\n",
    "wav_processor: str = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
    "sampling_rate: int = 16_000\n",
    "wav_max_length: int = 112_000 # 16_000 * 7, 7secs duration\n",
    "txt_processor: str = \"klue/bert-base\"\n",
    "txt_max_length: int = 64\n",
    "multilabel: bool = True\n",
    "load_from_cache_file: bool = True\n",
    "num_proc: int = 8\n",
    "batched: bool = True\n",
    "batch_size: int = 1000 # Not a torch batch_size\n",
    "writer_batch_size: int = 1000\n",
    "num_data: int = None\n",
    "preprocess: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:erc.datasets:Instantiate train Dataset\n"
     ]
    }
   ],
   "source": [
    "ds_kwargs = dict(\n",
    "    # Note for hard-coded kwargs\n",
    "    # generate_csv=False,\n",
    "    return_bio=False,\n",
    "    tokenizer_name=None,\n",
    "    max_length_wav=wav_max_length,\n",
    "    max_length_txt=txt_max_length,\n",
    "    multilabel=multilabel,\n",
    "    validation_fold=validation_fold,\n",
    "    mode=mode,\n",
    "    num_data=num_data,\n",
    "    # PRETRAINED_DATA_PATH=PRETRAINED_DATA_PATH,\n",
    ")\n",
    "\n",
    "ds = erc.datasets.KEMDDataset(**ds_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segment_id': 'Sess01_script01_M001',\n",
       " 'sampling_rate': 16000,\n",
       " 'wav': tensor([4.5776e-04, 1.5259e-04, 3.0518e-04,  ..., 6.1035e-05, 1.8311e-04,\n",
       "         2.7466e-04]),\n",
       " 'wav_mask': tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       " 'txt': '어 저 지그 지금 사람 친 거야? 지금 사람 친 거 맞지? 그치?\\n',\n",
       " 'txt_mask': None,\n",
       " 'emotion': array([0.9, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=object),\n",
       " 'valence': tensor(1.7000),\n",
       " 'arousal': tensor(4.),\n",
       " 'gender': tensor(0)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weight with Torchlightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from erc.\n",
    "\n",
    "WEIGHTS_PATH = '/home/hoesungryu/etri-erc/weights_AI_HUB/26908-valid_acc0.994.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
