{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "import torch.nn as  nn \n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from datasets import load_from_disk\n",
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import erc\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# device = torch.device('cuda:1')\n",
    "\n",
    "def get_label(batch: dict, task: erc.constants.Task = None):\n",
    "    device = torch.device('cuda:1')\n",
    "    # labels = batch[\"emotion\"].long()\n",
    "    # labels = torch.stack([batch[\"valence\"], batch[\"arousal\"]], dim=1).float()\n",
    "    labels = {\n",
    "    # \"emotion\": batch[\"emotion\"].to(device),\n",
    "    \"emotion\": batch[\"emotion\"].to(device),\n",
    "    \"regress\": torch.stack([batch[\"valence\"], batch[\"arousal\"]], dim=1),\n",
    "    \"vote_emotion\": batch.get(\"vote_emotion\", None)\n",
    "    }\n",
    "    # TODO: Add Multilabel Fetch\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _seed_everything(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "    # If the above line is uncommented, we get the following RuntimeError:\n",
    "    #  max_pool3d_with_indices_backward_cuda does not have a deterministic implementation\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "_seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 6\n",
    "valid_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_valid4_multilabelFalse_rdeuceTrue\")\n",
    "valid_dataloadaer = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_train4_multilabelFalse_rdeuceTrue\")\n",
    "train_dataloadaer = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# valid_dataset = load_from_disk(\"/home/hoesungryu/etri-erc/kemdy19-kemdy20_valid4_multilabelTrue_rdeuceFalse\")\n",
    "\n",
    "\n",
    "################\n",
    "# model load \n",
    "################\n",
    "with initialize(version_base=None, config_path=\"./config/model\"):\n",
    "    cfg = compose(config_name=\"mlp_mixer_roberta\")\n",
    "cfg.config['txt'] = \"klue/roberta-large\"\n",
    "# cfg['_target_'] = \"erc.model.inference_mlp_mixer.MLP_Mixer_Roberta\"\n",
    "cfg['_target_'] = \"erc.model.mlp_mixer.MLP_Mixer_Roberta\"\n",
    "\n",
    "\n",
    "# CKPT = '/home/hoesungryu/etri-erc/weights_AI_HUB/RobertaL_valid4_onehot_epoch25.ckpt'\n",
    "CKPT = '/home/hoesungryu/etri-erc/outputs/2023-04-08/13-23-20/61299-valid_acc0.949.ckpt'\n",
    "# CKPT = '/home/hoesungryu/etri-erc/outputs/2023-04-08/13-23-20/last.ckpt'\n",
    "SAVE_PATH = \"./RobertaL_valid_results\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "# ckpt = torch.load(CKPT, map_location=\"cpu\")\n",
    "ckpt = torch.load(CKPT, map_location = torch.device('cuda:1'))\n",
    "model_ckpt = ckpt.pop(\"state_dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from erc.constants import Task\n",
    "import erc\n",
    "\n",
    "\n",
    "logger = erc.utils.get_logger(__name__)\n",
    "\n",
    "\n",
    "pair = lambda x: x if isinstance(x, tuple) else (x, x)\n",
    "\n",
    "class PreNormResidual(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x)) + x\n",
    "\n",
    "def FeedForward(dim, expansion_factor = 4, dropout = 0., dense = nn.Linear):\n",
    "    inner_dim = int(dim * expansion_factor)\n",
    "    return nn.Sequential(\n",
    "        dense(dim, inner_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout),\n",
    "        dense(inner_dim, dim),\n",
    "        nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "def MLPMixer(*, image_size, channels, patch_size, dim, depth, num_classes, expansion_factor = 4, expansion_factor_token = 0.5, dropout = 0.):\n",
    "    image_h, image_w = pair(image_size)\n",
    "    assert (image_h % patch_size) == 0 and (image_w % patch_size) == 0, 'image must be divisible by patch size'\n",
    "    num_patches = (image_h // patch_size) * (image_w // patch_size)\n",
    "    chan_first, chan_last = partial(nn.Conv1d, kernel_size = 1), nn.Linear\n",
    "\n",
    "    return nn.Sequential(\n",
    "        Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "        nn.Linear((patch_size ** 2) * channels, dim),\n",
    "        *[nn.Sequential(\n",
    "            PreNormResidual(dim, FeedForward(num_patches, expansion_factor, dropout, chan_first)),\n",
    "            PreNormResidual(dim, FeedForward(dim, expansion_factor_token, dropout, chan_last))\n",
    "        ) for _ in range(depth)],\n",
    "        nn.LayerNorm(dim),\n",
    "        Reduce('b n c -> b c', 'mean'),\n",
    "        nn.Linear(dim, num_classes)\n",
    "    )\n",
    "class MLP_Mixer_Roberta(nn.Module):\n",
    "    TASK = Task.ALL\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: str,\n",
    "        # criterions: torch.nn.Module,\n",
    "        cls_coef: float = 0.5,\n",
    "        **config_kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.wav_model = Wav2Vec2ForSequenceClassification.from_pretrained(config['wav'])\n",
    "        self.txt_model = RobertaForSequenceClassification.from_pretrained(config['txt'])\n",
    "\n",
    "        proj_size = self.wav_model.config.classifier_proj_size\n",
    "        self.mlp_mixer = MLPMixer(image_size=proj_size,\n",
    "                                  **config['mlp_mixer'])\n",
    "        self.wav_projector = nn.Linear(self.wav_model.config.hidden_size, proj_size)\n",
    "        last_hdn_size = {\n",
    "            \"klue/roberta-base\": 768, \"klue/roberta-large\": 1024\n",
    "        }[config[\"txt\"]]\n",
    "        self.txt_projector = nn.Linear(last_hdn_size, proj_size)\n",
    "        self.gender_embed = nn.Embedding(num_embeddings=2, embedding_dim=proj_size)\n",
    "        self.use_gender = config_kwargs.get(\"use_gender\", False)\n",
    "        self.wav_gender = config_kwargs.get(\"wav_gender\", False)\n",
    "        self.txt_gender = config_kwargs.get(\"txt_gender\", False)\n",
    "        self.use_peakl = config_kwargs.get(\"use_peakl\", False)\n",
    "        self.wav_model = self.wav_model.wav2vec2\n",
    "        self.txt_model = self.txt_model.roberta\n",
    "\n",
    "        # self.criterions = criterions\n",
    "        if not (0 < cls_coef < 1):\n",
    "            cls_coef = 0.7\n",
    "        self.cls_coef = cls_coef\n",
    "        self.reg_coef = 1 - cls_coef\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        wav: torch.Tensor,\n",
    "        wav_mask: torch.Tensor,\n",
    "        txt: torch.Tensor,\n",
    "        txt_mask: torch.Tensor,\n",
    "        labels: torch.Tensor = None,\n",
    "        **kwargs\n",
    "    ) -> dict:\n",
    "        \"\"\" Size\n",
    "         WAV_hidden_dim: 1024\n",
    "         WAV_proj_size: 256\n",
    "         BERT_hidden_dim: 768\n",
    "         BERT_proj_size: 256\n",
    "        \"\"\"\n",
    "        # WAV \n",
    "        wav_outputs = self.wav_model(input_values=wav, attention_mask=wav_mask) # (B, S, WAV_hidden_dim)\n",
    "        hidden_states = self.wav_projector(wav_outputs[0]) # (B, S, WAV_proj_size) \n",
    "        # Pool hidden states. (B, proj_size)\n",
    "        if wav_mask is None:\n",
    "            pooled_wav_output = hidden_states.mean(dim=1)\n",
    "        else:\n",
    "            padding_mask = self.wav_model._get_feature_vector_attention_mask(hidden_states.shape[1], wav_mask)\n",
    "            hidden_states[~padding_mask] = 0.0\n",
    "            pooled_wav_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n",
    "\n",
    "        # TXT \n",
    "        txt_last_hidden_state = self.txt_model(input_ids=txt, attention_mask=txt_mask)[0] # (B, RoBERTa_hidden_dim)\n",
    "        txt_outputs = txt_last_hidden_state[:, 0, :]\n",
    "        pooled_txt_output = self.txt_projector(txt_outputs) # (B, RoBERTa_proj_size)\n",
    "\n",
    "        # (B, 1 , WAV_proj_size, BERT_proj_size)\n",
    "        matmul_output = torch.bmm(pooled_wav_output.unsqueeze(2), pooled_txt_output.unsqueeze(1)).unsqueeze(1)\n",
    "        logits = self.mlp_mixer(matmul_output) # (B, num_labels)\n",
    "\n",
    "        # calcuate the loss fct\n",
    "        cls_logits = logits[:, :-2]\n",
    "        cls_labels = labels[\"emotion\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"emotion\": cls_labels.detach(),\n",
    "            \"cls_pred\": cls_logits.detach(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_Mixer_Roberta(cfg.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(k):\n",
    "    _k = k.split(\".\")[1:]\n",
    "    return \".\".join(_k)\n",
    "new_ckpt = {parser(k): v for k, v in model_ckpt.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_deuce(outputs: dict) -> dict:\n",
    "        \"\"\" Find deuced emotions and remove from batch \"\"\"\n",
    "        result = outputs\n",
    "        emotion = outputs[\"emotion\"]\n",
    "        if emotion.ndim == 2:\n",
    "            # For multi-dimensional emotion cases\n",
    "            _, num_class = emotion.shape\n",
    "            v, _ = emotion.max(dim=1)\n",
    "            v = v.unsqueeze(dim=1).repeat(1, num_class)\n",
    "            um = (emotion == v).sum(dim=1) == 1 # (bsz, ), unique mask\n",
    "            if (um.sum() == 0).item():\n",
    "                # If every batches had deuce data\n",
    "                # Return scalar metrics only (removing cls/reg pred and logits)\n",
    "                result = {k: _v for k, _v in outputs.items() if _v.ndim == 0}\n",
    "            else:\n",
    "                result.update(\n",
    "                    {k: _v[um] for k, _v in outputs.items() if _v.ndim > 0}\n",
    "                )\n",
    "                result[\"emotion\"] = result[\"emotion\"].argmax(dim=1)\n",
    "            return result\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sort_outputs(outputs):\n",
    "        try:\n",
    "            result = dict()\n",
    "            keys: list = outputs[0].keys()\n",
    "            for key in keys:\n",
    "                data = outputs[0][key]\n",
    "                if data.ndim == 0:\n",
    "                    # Scalar value result\n",
    "                    result[key] = torch.stack([o[key] for o in outputs if key in o])\n",
    "                elif data.ndim in [1, 2]:\n",
    "                    # Batched \n",
    "                    result[key] = torch.concat([o[key] for o in outputs if key in o])\n",
    "        except AttributeError:\n",
    "            logger.warn(\"Error provoking data %s\", outputs)\n",
    "            breakpoint()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy, AUROC, ConcordanceCorrCoef, F1Score\n",
    "\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "model.to(device).load_state_dict(new_ckpt)\n",
    "\n",
    "\n",
    "acc = Accuracy(task=\"multiclass\", num_classes=7).to(device)\n",
    "\n",
    "pbar = tqdm(\n",
    "total=int(len(train_dataset)/BATCH_SIZE), \n",
    "iterable = enumerate(train_dataloadaer))\n",
    "\n",
    "for batch_idx, batch in pbar:\n",
    "    labels = get_label(batch) # concat \n",
    "    \n",
    "    result = model(wav=batch[\"wav\"].to(device),\n",
    "            wav_mask=batch[\"wav_mask\"].to(device),\n",
    "            txt=batch[\"txt\"].to(device),\n",
    "            txt_mask=batch[\"txt_mask\"].to(device),\n",
    "            labels=labels)\n",
    "    acc_ = acc(preds=result[\"cls_pred\"], target=result[\"emotion\"])\n",
    "\n",
    "model.eval()\n",
    "pbar_2 = tqdm(\n",
    "total=int(len(valid_dataset)/BATCH_SIZE), \n",
    "iterable = enumerate(valid_dataloadaer))\n",
    "for batch_idx, batch in pbar_2:\n",
    "    labels = get_label(batch) # concat \n",
    "    \n",
    "    result = model(wav=batch[\"wav\"].to(device),\n",
    "            wav_mask=batch[\"wav_mask\"].to(device),\n",
    "            txt=batch[\"txt\"].to(device),\n",
    "            txt_mask=batch[\"txt_mask\"].to(device),\n",
    "            labels=labels)\n",
    "    acc_ = acc(preds=result[\"cls_pred\"], target=result[\"emotion\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ = acc.compute()\n",
    "print(f\"Accuracy on all data: {acc_}\")\n",
    "    # result = remove_deuce(outputs=result)\n",
    "    # result[\"emotion\"] = result[\"emotion\"].argmax(dim=1)\n",
    "\n",
    "    # pred.append(list(result[\"cls_pred\"].detach().cpu().numpy()))\n",
    "    # target.append(list(result[\"emotion\"].detach().cpu().numpy()))\n",
    "    # ACC_score = acc(preds=result[\"cls_pred\"], target=result[\"emotion\"]).item()\n",
    "#     print(f\"{ACC_score:.4f}\")\n",
    "#     print(torch.sum(result[\"cls_pred\"] == result[\"emotion\"]))\n",
    "    # break\n",
    "#     break\n",
    "#     total_score.append(ACC_score)\n",
    "    # save_name = os.path.join(SAVE_PATH, f'wav_txt_{batch_idx:03d}.pickle')\n",
    "    # with open(save_name, 'wb') as f:\n",
    "    #     pickle.dump(save_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "    # total += result[\"emotion\"].size(0)\n",
    "    # correct += torch.sum(result[\"cls_pred\"].argmax(dim=1) == result[\"emotion\"]).item()\n",
    "\n",
    "    # accumulate += (correct / total)\n",
    "    # accumulate /= 2 \n",
    "# print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "#     100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ffc592dff13d4d41f223053abde961ed0db53f702fad28eadf2c0045c5935a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
