{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:erc.utils:Instantiate KEMDy19 Dataset\n",
      "INFO:erc.utils:./data/kemdy19.csv does not exists. Process from raw data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1da6964fc94265a6ade65faf950909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing ECG / EDA / Label from /home/hoesungryu/workspace:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:erc.utils:New dataframe saved as data/kemdy19.csv\n"
     ]
    },
    {
     "ename": "InstantiationException",
     "evalue": "Error in call to target 'erc.datasets.KEMDy19Dataset':\nOSError(\"Cannot save file into a non-existent directory: 'data'\")\nfull_key: dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py:92\u001b[0m, in \u001b[0;36m_call_target\u001b[0;34m(_target_, _partial_, args, kwargs, full_key)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m _target_(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/etri-erc/erc/datasets.py:270\u001b[0m, in \u001b[0;36mKEMDy19Dataset.__init__\u001b[0;34m(self, base_path, generate_csv, return_bio, max_length_wav, max_length_txt, tokenizer_name, validation_fold, mode)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    260\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    261\u001b[0m     base_path: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./data/KEMDy19\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m     mode: RunMode \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m RunMode\u001b[39m.\u001b[39mTRAIN\n\u001b[1;32m    269\u001b[0m ):\n\u001b[0;32m--> 270\u001b[0m     \u001b[39msuper\u001b[39;49m(KEMDy19Dataset, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    271\u001b[0m         base_path,\n\u001b[1;32m    272\u001b[0m         generate_csv,\n\u001b[1;32m    273\u001b[0m         return_bio,\n\u001b[1;32m    274\u001b[0m         max_length_wav,\n\u001b[1;32m    275\u001b[0m         max_length_txt,\n\u001b[1;32m    276\u001b[0m         tokenizer_name,\n\u001b[1;32m    277\u001b[0m         validation_fold,\n\u001b[1;32m    278\u001b[0m         mode\n\u001b[1;32m    279\u001b[0m     )\n",
      "File \u001b[0;32m~/etri-erc/erc/datasets.py:68\u001b[0m, in \u001b[0;36mKEMDBase.__init__\u001b[0;34m(self, base_path, generate_csv, return_bio, max_length_wav, max_length_txt, tokenizer_name, validation_fold, mode)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m RunMode[mode\u001b[39m.\u001b[39mupper()] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mode, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m mode\n\u001b[0;32m---> 68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf: pd\u001b[39m.\u001b[39mDataFrame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessed_db(generate_csv\u001b[39m=\u001b[39;49mgenerate_csv,\n\u001b[1;32m     69\u001b[0m                                           fold_num\u001b[39m=\u001b[39;49mvalidation_fold)\n",
      "File \u001b[0;32m~/etri-erc/erc/datasets.py:199\u001b[0m, in \u001b[0;36mKEMDBase.processed_db\u001b[0;34m(self, generate_csv, fold_num)\u001b[0m\n\u001b[1;32m    198\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTOTAL_DF_PATH\u001b[39m}\u001b[39;00m\u001b[39m does not exists. Process from raw data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m     total_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmerge_csv(base_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_path, save_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mTOTAL_DF_PATH)\n\u001b[1;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/etri-erc/erc/datasets.py:286\u001b[0m, in \u001b[0;36mKEMDy19Dataset.merge_csv\u001b[0;34m(self, base_path, save_path)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge_csv\u001b[39m(\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    283\u001b[0m     base_path: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m Path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./data/KEMDy19\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    284\u001b[0m     save_path: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m Path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./data/kemdy19.csv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    285\u001b[0m ):\n\u001b[0;32m--> 286\u001b[0m     \u001b[39mreturn\u001b[39;00m merge_csv_kemdy19(base_path\u001b[39m=\u001b[39;49mbase_path, save_path\u001b[39m=\u001b[39;49msave_path)\n",
      "File \u001b[0;32m~/etri-erc/erc/preprocess.py:93\u001b[0m, in \u001b[0;36mmerge_csv_kemdy19\u001b[0;34m(base_path, save_path)\u001b[0m\n\u001b[1;32m     92\u001b[0m total_df \u001b[39m=\u001b[39m total_df[\u001b[39m~\u001b[39mtotal_df[\u001b[39m'\u001b[39m\u001b[39memotion\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcontains(\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m---> 93\u001b[0m total_df\u001b[39m.\u001b[39;49mto_csv(save_path, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m total_df\n",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/pandas/core/generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3711\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3712\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3713\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3717\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3718\u001b[0m )\n\u001b[0;32m-> 3720\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3721\u001b[0m     path_or_buf,\n\u001b[1;32m   3722\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[1;32m   3723\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3724\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3725\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3726\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3727\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3728\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3729\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3730\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3731\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3732\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3733\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3734\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3735\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3736\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3737\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/pandas/io/formats/format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1170\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1171\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1172\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1188\u001b[0m )\n\u001b[0;32m-> 1189\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[1;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[1;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/pandas/io/common.py:734\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[0;32m--> 734\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[1;32m    736\u001b[0m \u001b[39mif\u001b[39;00m compression:\n",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/pandas/io/common.py:597\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInstantiationException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     cfg \u001b[39m=\u001b[39m hydra\u001b[39m.\u001b[39mcompose(config_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m, overrides\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdataset._target_=erc.datasets.KEMDy19Dataset\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m     24\u001b[0m \u001b[39m# select 1-fold \u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m train_dataset \u001b[39m=\u001b[39m hydra\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49minstantiate(cfg\u001b[39m.\u001b[39;49mdataset, base_path \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/home/hoesungryu/workspace/\u001b[39;49m\u001b[39m\"\u001b[39;49m , mode \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py:226\u001b[0m, in \u001b[0;36minstantiate\u001b[0;34m(config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     _convert_ \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mpop(_Keys\u001b[39m.\u001b[39mCONVERT, ConvertMode\u001b[39m.\u001b[39mNONE)\n\u001b[1;32m    224\u001b[0m     _partial_ \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mpop(_Keys\u001b[39m.\u001b[39mPARTIAL, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m instantiate_node(\n\u001b[1;32m    227\u001b[0m         config, \u001b[39m*\u001b[39;49margs, recursive\u001b[39m=\u001b[39;49m_recursive_, convert\u001b[39m=\u001b[39;49m_convert_, partial\u001b[39m=\u001b[39;49m_partial_\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m \u001b[39melif\u001b[39;00m OmegaConf\u001b[39m.\u001b[39mis_list(config):\n\u001b[1;32m    230\u001b[0m     \u001b[39m# Finalize config (convert targets to strings, merge with kwargs)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     config_copy \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py:347\u001b[0m, in \u001b[0;36minstantiate_node\u001b[0;34m(node, convert, recursive, partial, *args)\u001b[0m\n\u001b[1;32m    342\u001b[0m                 value \u001b[39m=\u001b[39m instantiate_node(\n\u001b[1;32m    343\u001b[0m                     value, convert\u001b[39m=\u001b[39mconvert, recursive\u001b[39m=\u001b[39mrecursive\n\u001b[1;32m    344\u001b[0m                 )\n\u001b[1;32m    345\u001b[0m             kwargs[key] \u001b[39m=\u001b[39m _convert_node(value, convert)\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mreturn\u001b[39;00m _call_target(_target_, partial, args, kwargs, full_key)\n\u001b[1;32m    348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[39m# If ALL or PARTIAL non structured or OBJECT non structured,\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[39m# instantiate in dict and resolve interpolations eagerly.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[39mif\u001b[39;00m convert \u001b[39m==\u001b[39m ConvertMode\u001b[39m.\u001b[39mALL \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    352\u001b[0m         convert \u001b[39min\u001b[39;00m (ConvertMode\u001b[39m.\u001b[39mPARTIAL, ConvertMode\u001b[39m.\u001b[39mOBJECT)\n\u001b[1;32m    353\u001b[0m         \u001b[39mand\u001b[39;00m node\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39mobject_type \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39mdict\u001b[39m)\n\u001b[1;32m    354\u001b[0m     ):\n",
      "File \u001b[0;32m~/.conda/envs/erc/lib/python3.10/site-packages/hydra/_internal/instantiate/_instantiate2.py:97\u001b[0m, in \u001b[0;36m_call_target\u001b[0;34m(_target_, _partial_, args, kwargs, full_key)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m full_key:\n\u001b[1;32m     96\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mfull_key: \u001b[39m\u001b[39m{\u001b[39;00mfull_key\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 97\u001b[0m \u001b[39mraise\u001b[39;00m InstantiationException(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mInstantiationException\u001b[0m: Error in call to target 'erc.datasets.KEMDy19Dataset':\nOSError(\"Cannot save file into a non-existent directory: 'data'\")\nfull_key: dataset"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import hydra\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import colormaps\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "from erc import drawing_ellipse, split_df_by_gender\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "with hydra.initialize(version_base=None, config_path=\"./config\"):\n",
    "    cfg = hydra.compose(config_name=\"config\", overrides={\"dataset._target_=erc.datasets.KEMDy19Dataset\"})\n",
    "\n",
    "# select 1-fold \n",
    "train_dataset = hydra.utils.instantiate(cfg.dataset, mode = \"train\")\n",
    "# valid_dataset = hydra.utils.instantiate(cfg.dataset, mode = \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "from jiwer import wer # wer metircs \n",
    "from transformers import Wav2Vec2Processor\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at kresnik/wav2vec2-large-xlsr-korean were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at kresnik/wav2vec2-large-xlsr-korean and are newly initialized: ['classifier.bias', 'projector.weight', 'projector.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Pre-training Scheme ... \n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "pretrain_str = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
    "# pretrain_str = \"w11wo/wav2vec2-xls-r-300m-korean\"\n",
    "\n",
    "processor= Wav2Vec2Processor.from_pretrained(pretrain_str)\n",
    "pretrained_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    # \"wav2vec2-xls-r-300m-korean\",\n",
    "    pretrain_str,\n",
    "    num_labels=7\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315702919"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import erc\n",
    "\n",
    "erc.utils.count_parameters(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoesungryu/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = pretrained_model.to(device)\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5,  eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1555280685424805\n",
      "1.876920461654663\n",
      "2.027853488922119\n",
      "2.0851197242736816\n",
      "1.8583887815475464\n",
      "2.0513827800750732\n",
      "2.1173458099365234\n",
      "2.0462806224823\n",
      "1.9930295944213867\n",
      "2.056407928466797\n",
      "1.8575515747070312\n",
      "1.8788491487503052\n",
      "1.8656747341156006\n",
      "1.8904151916503906\n",
      "1.7740397453308105\n",
      "1.8491510152816772\n",
      "1.8243505954742432\n",
      "1.8593350648880005\n",
      "1.8837928771972656\n",
      "1.7847356796264648\n",
      "1.9228465557098389\n",
      "1.8982136249542236\n",
      "2.12068772315979\n",
      "1.873547911643982\n",
      "1.7592494487762451\n",
      "1.9251984357833862\n",
      "2.132913112640381\n",
      "2.089188575744629\n",
      "2.152311086654663\n",
      "2.1025381088256836\n",
      "2.107727527618408\n",
      "2.067348003387451\n",
      "2.1951334476470947\n",
      "2.1749963760375977\n",
      "1.983643889427185\n",
      "2.0064048767089844\n",
      "1.9801409244537354\n",
      "1.9811813831329346\n",
      "2.04870343208313\n",
      "1.6701644659042358\n",
      "1.9914988279342651\n",
      "1.8618470430374146\n",
      "1.7785487174987793\n",
      "1.7649991512298584\n",
      "1.8274312019348145\n",
      "1.9521520137786865\n",
      "2.119351387023926\n",
      "1.9559354782104492\n",
      "1.8991422653198242\n",
      "1.9091132879257202\n",
      "1.9706895351409912\n",
      "1.881345510482788\n",
      "2.113330364227295\n",
      "1.9323103427886963\n",
      "1.6099073886871338\n",
      "1.4641804695129395\n",
      "2.0849404335021973\n",
      "1.8210415840148926\n",
      "1.3546903133392334\n",
      "1.5381919145584106\n",
      "1.2546331882476807\n",
      "1.6515271663665771\n",
      "1.4390521049499512\n",
      "1.4468591213226318\n",
      "1.6246349811553955\n",
      "1.583942174911499\n",
      "1.280651569366455\n",
      "1.7488924264907837\n",
      "1.7078145742416382\n",
      "1.4900660514831543\n",
      "1.4115403890609741\n",
      "1.563035488128662\n",
      "1.7343027591705322\n",
      "1.0259652137756348\n",
      "1.0873494148254395\n",
      "1.2184253931045532\n",
      "1.4280909299850464\n",
      "0.9484652280807495\n",
      "1.3497849702835083\n",
      "1.1476106643676758\n",
      "1.335889458656311\n",
      "1.02734375\n",
      "0.9962484836578369\n",
      "1.6246323585510254\n",
      "1.597988486289978\n",
      "1.6635334491729736\n",
      "2.57802677154541\n",
      "2.016523838043213\n",
      "1.5835554599761963\n",
      "1.0226426124572754\n",
      "2.1017398834228516\n",
      "2.005934953689575\n",
      "1.906479001045227\n",
      "1.4081907272338867\n",
      "1.4420689344406128\n",
      "1.3558300733566284\n",
      "1.911588191986084\n",
      "1.210157871246338\n",
      "2.4431920051574707\n",
      "2.1948013305664062\n",
      "2.2849974632263184\n",
      "2.610792636871338\n",
      "1.8133714199066162\n",
      "2.3049213886260986\n",
      "2.3221113681793213\n",
      "2.457827568054199\n",
      "2.2631115913391113\n",
      "2.2480812072753906\n",
      "2.4038033485412598\n",
      "2.3148977756500244\n",
      "2.229520797729492\n",
      "2.256654739379883\n",
      "2.207669496536255\n",
      "2.098263740539551\n",
      "1.5157562494277954\n",
      "1.9091832637786865\n",
      "1.7102019786834717\n",
      "2.3036422729492188\n",
      "1.5805141925811768\n",
      "1.457560420036316\n",
      "1.904937505722046\n",
      "1.6048084497451782\n",
      "2.8150525093078613\n",
      "2.1851940155029297\n",
      "1.734528660774231\n",
      "2.062833547592163\n",
      "2.2167446613311768\n",
      "2.5343637466430664\n",
      "2.411458730697632\n",
      "2.1690406799316406\n",
      "2.128588914871216\n",
      "2.1434364318847656\n",
      "1.1091196537017822\n",
      "2.1637496948242188\n",
      "2.1756162643432617\n",
      "1.7665607929229736\n",
      "2.1064565181732178\n",
      "1.1762598752975464\n",
      "1.3875658512115479\n",
      "1.1442975997924805\n",
      "1.2124907970428467\n",
      "1.9479939937591553\n",
      "1.6245278120040894\n",
      "2.0350351333618164\n",
      "1.7020952701568604\n",
      "1.5355252027511597\n",
      "1.6414711475372314\n",
      "1.649693489074707\n",
      "1.5056204795837402\n",
      "1.9449706077575684\n",
      "1.9135944843292236\n",
      "1.913515567779541\n",
      "1.0976333618164062\n",
      "1.4294203519821167\n",
      "1.3894110918045044\n",
      "1.4778399467468262\n",
      "1.4942166805267334\n",
      "1.4972755908966064\n",
      "1.3472310304641724\n",
      "1.4565696716308594\n",
      "1.9450939893722534\n",
      "1.3673187494277954\n",
      "1.3944737911224365\n",
      "0.9960724115371704\n",
      "2.0005459785461426\n",
      "1.4483907222747803\n",
      "2.1015121936798096\n",
      "2.0948400497436523\n",
      "2.1818654537200928\n",
      "2.148501396179199\n",
      "2.107664108276367\n",
      "2.1262588500976562\n",
      "1.469200611114502\n",
      "0.8492907881736755\n",
      "0.9003651142120361\n",
      "1.7550184726715088\n",
      "1.551711916923523\n",
      "1.5845272541046143\n",
      "0.9780263304710388\n",
      "1.2820305824279785\n",
      "2.380551338195801\n",
      "1.7607169151306152\n",
      "2.332958936691284\n",
      "1.4904356002807617\n",
      "2.010730743408203\n",
      "1.5818393230438232\n",
      "2.3360702991485596\n",
      "1.5703871250152588\n",
      "2.1001110076904297\n",
      "1.268540620803833\n",
      "1.5523734092712402\n",
      "1.6137958765029907\n",
      "2.803616523742676\n",
      "1.6967144012451172\n",
      "2.042360305786133\n",
      "1.932665467262268\n",
      "1.4546728134155273\n",
      "1.6323151588439941\n",
      "1.6050232648849487\n",
      "2.165898561477661\n",
      "1.3026254177093506\n",
      "2.1598668098449707\n",
      "2.120731830596924\n",
      "1.9952763319015503\n",
      "2.1577234268188477\n",
      "2.021146297454834\n",
      "1.85026216506958\n",
      "2.093170642852783\n",
      "2.1879682540893555\n",
      "2.0312275886535645\n",
      "2.0354199409484863\n",
      "2.067168712615967\n",
      "2.2082760334014893\n",
      "2.2039315700531006\n",
      "1.8728489875793457\n",
      "1.9254499673843384\n",
      "1.8582797050476074\n",
      "1.8680423498153687\n",
      "2.0910604000091553\n",
      "1.909313678741455\n",
      "1.8841917514801025\n",
      "1.8420517444610596\n",
      "1.8698151111602783\n",
      "1.7751734256744385\n",
      "1.9482262134552002\n",
      "2.00065279006958\n",
      "2.522268533706665\n",
      "2.2306971549987793\n",
      "1.6145473718643188\n",
      "1.7663953304290771\n",
      "1.7952322959899902\n",
      "1.7658522129058838\n",
      "1.829628348350525\n",
      "1.5123188495635986\n",
      "1.933901309967041\n",
      "1.977644920349121\n",
      "1.9600650072097778\n",
      "2.0439043045043945\n",
      "2.0289082527160645\n",
      "1.8813846111297607\n",
      "1.899658441543579\n",
      "1.856689214706421\n",
      "1.9337234497070312\n",
      "1.8622846603393555\n",
      "1.822402834892273\n",
      "1.651320219039917\n",
      "1.635329008102417\n",
      "2.1807351112365723\n",
      "1.69287109375\n",
      "1.645535945892334\n",
      "1.6065897941589355\n",
      "1.649062156677246\n",
      "1.6307692527770996\n",
      "1.6545333862304688\n",
      "2.5816731452941895\n",
      "2.392512559890747\n",
      "2.7754454612731934\n",
      "1.6915900707244873\n",
      "3.082465171813965\n",
      "2.1816585063934326\n",
      "1.5598223209381104\n",
      "1.5991367101669312\n",
      "2.1247315406799316\n",
      "1.7961126565933228\n",
      "2.337031602859497\n",
      "1.7594821453094482\n",
      "2.799046039581299\n",
      "2.042682647705078\n",
      "1.8133809566497803\n",
      "2.4353315830230713\n",
      "1.7769076824188232\n",
      "2.309659957885742\n",
      "1.803133487701416\n",
      "2.215506076812744\n",
      "2.495760679244995\n",
      "2.1549458503723145\n",
      "2.3532896041870117\n",
      "2.4149837493896484\n",
      "2.1655287742614746\n",
      "2.1520049571990967\n",
      "2.130939245223999\n",
      "1.5195060968399048\n",
      "1.593585729598999\n",
      "1.642869234085083\n",
      "1.6102664470672607\n",
      "1.6694620847702026\n",
      "1.6462866067886353\n",
      "1.560239315032959\n",
      "1.60585355758667\n",
      "1.5100574493408203\n",
      "1.7008287906646729\n",
      "1.6656951904296875\n",
      "1.6371064186096191\n",
      "1.5238089561462402\n",
      "1.6533548831939697\n",
      "1.6558771133422852\n",
      "1.7100715637207031\n",
      "1.2994110584259033\n",
      "1.2777713537216187\n",
      "1.4285250902175903\n",
      "1.8344781398773193\n",
      "1.4231395721435547\n",
      "1.5815590620040894\n",
      "1.432011365890503\n",
      "1.5182156562805176\n",
      "1.732467532157898\n",
      "1.701855182647705\n",
      "1.5647544860839844\n",
      "1.460197925567627\n",
      "1.4193601608276367\n",
      "1.5724735260009766\n",
      "1.2076836824417114\n",
      "1.516430139541626\n",
      "1.5008878707885742\n",
      "1.215425729751587\n",
      "1.6230236291885376\n",
      "1.7173484563827515\n",
      "1.7976100444793701\n",
      "1.0905264616012573\n",
      "1.0632219314575195\n",
      "1.5550299882888794\n",
      "1.5081956386566162\n",
      "1.5231037139892578\n",
      "1.6149630546569824\n",
      "1.3937560319900513\n",
      "1.9441900253295898\n",
      "1.4105076789855957\n",
      "1.5431957244873047\n",
      "1.9128646850585938\n",
      "1.0203204154968262\n",
      "1.329235553741455\n",
      "0.9824995994567871\n",
      "1.340240478515625\n",
      "1.2983083724975586\n",
      "1.2916040420532227\n",
      "1.300654649734497\n",
      "1.2443894147872925\n",
      "1.3297467231750488\n",
      "1.3011749982833862\n",
      "1.2235081195831299\n",
      "1.660060167312622\n",
      "1.636967658996582\n",
      "1.6772708892822266\n",
      "0.9277613162994385\n",
      "0.8301488161087036\n",
      "1.2907261848449707\n",
      "1.1741302013397217\n",
      "1.1611210107803345\n",
      "1.1603937149047852\n",
      "1.4875351190567017\n",
      "1.130158543586731\n",
      "0.8755053281784058\n",
      "1.070465326309204\n",
      "1.13592529296875\n",
      "0.8328679203987122\n",
      "0.8540888428688049\n",
      "0.8727591633796692\n",
      "0.8977503776550293\n",
      "0.7192800045013428\n",
      "1.6703680753707886\n",
      "2.4842538833618164\n",
      "1.4833903312683105\n",
      "1.5609028339385986\n",
      "1.4898972511291504\n",
      "1.4231377840042114\n",
      "1.5767573118209839\n",
      "2.3691630363464355\n",
      "1.6613713502883911\n",
      "1.591747760772705\n",
      "2.1109628677368164\n",
      "0.9226978421211243\n",
      "0.6544594764709473\n",
      "1.474859356880188\n",
      "1.6497682332992554\n",
      "2.4473013877868652\n",
      "2.8779659271240234\n",
      "2.6362318992614746\n",
      "2.261159896850586\n",
      "0.5507780313491821\n",
      "0.549339771270752\n",
      "0.625969648361206\n",
      "2.144735336303711\n",
      "2.7359910011291504\n",
      "1.389082670211792\n",
      "1.4122660160064697\n",
      "1.9473471641540527\n",
      "1.384636402130127\n",
      "1.5233166217803955\n",
      "1.8805855512619019\n",
      "1.9282500743865967\n",
      "1.8595964908599854\n",
      "1.916464924812317\n",
      "1.9602015018463135\n",
      "2.1848275661468506\n",
      "2.1393842697143555\n",
      "1.8809901475906372\n",
      "2.0338149070739746\n",
      "1.138502836227417\n",
      "1.792614221572876\n",
      "1.6930649280548096\n",
      "1.6105681657791138\n",
      "1.6099352836608887\n",
      "0.7651305198669434\n",
      "1.9933104515075684\n",
      "1.8316564559936523\n",
      "1.9454574584960938\n",
      "1.9751665592193604\n",
      "1.6097966432571411\n",
      "1.9667028188705444\n",
      "0.936764121055603\n",
      "1.0304676294326782\n",
      "1.7476062774658203\n",
      "1.8685736656188965\n",
      "1.1448251008987427\n",
      "2.112450361251831\n",
      "2.161198139190674\n",
      "0.9329229593276978\n",
      "1.0214686393737793\n",
      "1.1629725694656372\n",
      "1.9126983880996704\n",
      "2.034287214279175\n",
      "1.9684932231903076\n",
      "3.2298696041107178\n",
      "1.9614132642745972\n",
      "2.3210508823394775\n",
      "2.2678098678588867\n",
      "2.9998490810394287\n",
      "1.8333550691604614\n",
      "2.6780645847320557\n",
      "2.5939223766326904\n",
      "3.05678129196167\n",
      "2.1565723419189453\n",
      "2.9972007274627686\n",
      "2.059548854827881\n",
      "2.6802687644958496\n",
      "2.1445059776306152\n",
      "2.3455452919006348\n",
      "1.3937633037567139\n",
      "2.193197011947632\n",
      "2.1767196655273438\n",
      "1.3204940557479858\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     17\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m:input_values,\n\u001b[1;32m     18\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m:batch[\u001b[39m'\u001b[39m\u001b[39mwav_mask\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     20\u001b[0m inputs \u001b[39m=\u001b[39m {key: inputs[key]\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m inputs}\n\u001b[0;32m---> 23\u001b[0m logits \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     26\u001b[0m \u001b[39m# outputs = torch.argmax(logits, dim=-1)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# print(logi)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits, labels\u001b[39m.\u001b[39mlong())\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1814\u001b[0m, in \u001b[0;36mWav2Vec2ForSequenceClassification.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1811\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1812\u001b[0m output_hidden_states \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_weighted_layer_sum \u001b[39melse\u001b[39;00m output_hidden_states\n\u001b[0;32m-> 1814\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwav2vec2(\n\u001b[1;32m   1815\u001b[0m     input_values,\n\u001b[1;32m   1816\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1817\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1818\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1819\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1820\u001b[0m )\n\u001b[1;32m   1822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_weighted_layer_sum:\n\u001b[1;32m   1823\u001b[0m     hidden_states \u001b[39m=\u001b[39m outputs[_HIDDEN_STATES_START_POSITION]\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1315\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1310\u001b[0m hidden_states, extract_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_projection(extract_features)\n\u001b[1;32m   1311\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mask_hidden_states(\n\u001b[1;32m   1312\u001b[0m     hidden_states, mask_time_indices\u001b[39m=\u001b[39mmask_time_indices, attention_mask\u001b[39m=\u001b[39mattention_mask\n\u001b[1;32m   1313\u001b[0m )\n\u001b[0;32m-> 1315\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1316\u001b[0m     hidden_states,\n\u001b[1;32m   1317\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1318\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1319\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1320\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1321\u001b[0m )\n\u001b[1;32m   1323\u001b[0m hidden_states \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:888\u001b[0m, in \u001b[0;36mWav2Vec2EncoderStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    882\u001b[0m         layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    883\u001b[0m             create_custom_forward(layer),\n\u001b[1;32m    884\u001b[0m             hidden_states,\n\u001b[1;32m    885\u001b[0m             attention_mask,\n\u001b[1;32m    886\u001b[0m         )\n\u001b[1;32m    887\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         layer_outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    889\u001b[0m             hidden_states, attention_mask\u001b[39m=\u001b[39;49mattention_mask, output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    890\u001b[0m         )\n\u001b[1;32m    891\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    893\u001b[0m \u001b[39mif\u001b[39;00m skip_the_layer:\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:720\u001b[0m, in \u001b[0;36mWav2Vec2EncoderLayerStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    718\u001b[0m attn_residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    719\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 720\u001b[0m hidden_states, attn_weights, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    721\u001b[0m     hidden_states, attention_mask\u001b[39m=\u001b[39;49mattention_mask, output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    723\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    724\u001b[0m hidden_states \u001b[39m=\u001b[39m attn_residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:620\u001b[0m, in \u001b[0;36mWav2Vec2Attention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    616\u001b[0m     attn_weights_reshaped \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    618\u001b[0m attn_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(attn_weights, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m--> 620\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(attn_probs, value_states)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m attn_output\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim):\n\u001b[1;32m    623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    624\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`attn_output` should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz,\u001b[39m \u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\u001b[39m \u001b[39mtgt_len,\u001b[39m \u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m}\u001b[39;00m\u001b[39m, but is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mattn_output\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    626\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size= 2)\n",
    "\n",
    "\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, training_dataloader = accelerator.prepare(\n",
    "     model, optimizer, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_loss = 0\n",
    "train_acc_sum = 0\n",
    "train_loss = []\n",
    "for step, batch in enumerate(train_loader): \n",
    "    optimizer.zero_grad()\n",
    "    labels = (batch['emotion']).to(device)\n",
    "    input_values = processor(batch[\"wav\"],\n",
    "                             sampling_rate=16000,\n",
    "                             return_tensors=\"pt\",\n",
    "                             return_attention_mask = False)['input_values'].squeeze()\n",
    "    inputs = {\"input_values\":input_values,\n",
    "              \"attention_mask\":batch['wav_mask'],\n",
    "    }\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "\n",
    "\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "    \n",
    "    # outputs = torch.argmax(logits, dim=-1)\n",
    "    # print(logi)\n",
    "\n",
    "    loss = criterion(logits, labels.long())\n",
    "    total_loss += loss.item()\n",
    "    train_loss.append(total_loss/(step+1))\n",
    "    # print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "avg_train_loss = total_loss / len(train_loader)\n",
    "print(f'  Average training loss: {avg_train_loss:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape)\n",
    "print(labels.shape)\n",
    "print(labels.dtype)\n",
    "print(logits.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = model.config.id2label[predicted_class_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EmotionClassfierWithAudio(nn.Module):\n",
    "    def __init__(self,pretrained_model, n_classes=7):\n",
    "        super().__init__()\n",
    "        self.pretrianed_model = pretrained_model\n",
    "        # for p in self.pretrianed_model.parameters():\n",
    "            # p.requires_grad = False\n",
    "\n",
    "        self.fc = nn.Linear(1025,n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.adaptive_avg_pool3d(self.pretrianed_model(x).logits , axis=1).squeeze()# mean of 355 // logit output shape: B, Seq(355), 1025\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jiwer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "# import soundfile as sf\n",
    "import torch\n",
    "from jiwer import wer # wer metircs \n",
    "from transformers import Wav2Vec2Processor\n",
    "batch = train_dataset[2]\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "inputs = processor(batch[\"wav\"], sampling_rate=16000, return_tensors=\"pt\", return_attention_mask = False,\n",
    "                #    padding=\"longest\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['attention_mask'].squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\").to('cuda')\n",
    "\n",
    "input_values = inputs.input_values\n",
    "print(input_values.shape)\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values.type(torch.FloatTensor).to(\"cuda\")).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(batch['txt'])\n",
    "print(transcription)\n",
    "print(\"WER:\", wer(batch['txt'], transcription))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(input_values.type(torch.FloatTensor).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d91dba4ef89c422bc4f5a90183fbfeb50a0fdb361fb291167f0137a8b87ec3ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
