{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:erc.utils:Instantiate KEMDy19 Dataset\n",
      "INFO:erc.utils:Instantiate KEMDy19 Dataset\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import hydra\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import colormaps\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "from erc import drawing_ellipse, split_df_by_gender\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "with hydra.initialize(version_base=None, config_path=\"./config\"):\n",
    "    cfg = hydra.compose(config_name=\"config\", overrides={\"dataset._target_=erc.datasets.KEMDy19Dataset\"})\n",
    "# select 1-fold \n",
    "cfg.dataset.mode = \"train\"\n",
    "train_dataset = hydra.utils.instantiate(cfg.dataset)\n",
    "\n",
    "cfg.dataset.mode = \"valid\"\n",
    "cfg.dataset.validation_fold = 0\n",
    "valid_dataset = hydra.utils.instantiate(cfg.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "from jiwer import wer # wer metircs \n",
    "from transformers import Wav2Vec2Processor\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at kresnik/wav2vec2-large-xlsr-korean were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at kresnik/wav2vec2-large-xlsr-korean and are newly initialized: ['classifier.bias', 'projector.weight', 'projector.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Pre-training Scheme ... \n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "pretrain_str = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
    "# pretrain_str = \"w11wo/wav2vec2-xls-r-300m-korean\"\n",
    "\n",
    "processor= Wav2Vec2Processor.from_pretrained(pretrain_str)\n",
    "pretrained_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    # \"wav2vec2-xls-r-300m-korean\",\n",
    "    pretrain_str,\n",
    "    num_labels=7\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315702919"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import erc\n",
    "\n",
    "erc.utils.count_parameters(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoesungryu/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = pretrained_model.to(device)\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5,  eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1555280685424805\n",
      "1.876920461654663\n",
      "2.027853488922119\n",
      "2.0851197242736816\n",
      "1.8583887815475464\n",
      "2.0513827800750732\n",
      "2.1173458099365234\n",
      "2.0462806224823\n",
      "1.9930295944213867\n",
      "2.056407928466797\n",
      "1.8575515747070312\n",
      "1.8788491487503052\n",
      "1.8656747341156006\n",
      "1.8904151916503906\n",
      "1.7740397453308105\n",
      "1.8491510152816772\n",
      "1.8243505954742432\n",
      "1.8593350648880005\n",
      "1.8837928771972656\n",
      "1.7847356796264648\n",
      "1.9228465557098389\n",
      "1.8982136249542236\n",
      "2.12068772315979\n",
      "1.873547911643982\n",
      "1.7592494487762451\n",
      "1.9251984357833862\n",
      "2.132913112640381\n",
      "2.089188575744629\n",
      "2.152311086654663\n",
      "2.1025381088256836\n",
      "2.107727527618408\n",
      "2.067348003387451\n",
      "2.1951334476470947\n",
      "2.1749963760375977\n",
      "1.983643889427185\n",
      "2.0064048767089844\n",
      "1.9801409244537354\n",
      "1.9811813831329346\n",
      "2.04870343208313\n",
      "1.6701644659042358\n",
      "1.9914988279342651\n",
      "1.8618470430374146\n",
      "1.7785487174987793\n",
      "1.7649991512298584\n",
      "1.8274312019348145\n",
      "1.9521520137786865\n",
      "2.119351387023926\n",
      "1.9559354782104492\n",
      "1.8991422653198242\n",
      "1.9091132879257202\n",
      "1.9706895351409912\n",
      "1.881345510482788\n",
      "2.113330364227295\n",
      "1.9323103427886963\n",
      "1.6099073886871338\n",
      "1.4641804695129395\n",
      "2.0849404335021973\n",
      "1.8210415840148926\n",
      "1.3546903133392334\n",
      "1.5381919145584106\n",
      "1.2546331882476807\n",
      "1.6515271663665771\n",
      "1.4390521049499512\n",
      "1.4468591213226318\n",
      "1.6246349811553955\n",
      "1.583942174911499\n",
      "1.280651569366455\n",
      "1.7488924264907837\n",
      "1.7078145742416382\n",
      "1.4900660514831543\n",
      "1.4115403890609741\n",
      "1.563035488128662\n",
      "1.7343027591705322\n",
      "1.0259652137756348\n",
      "1.0873494148254395\n",
      "1.2184253931045532\n",
      "1.4280909299850464\n",
      "0.9484652280807495\n",
      "1.3497849702835083\n",
      "1.1476106643676758\n",
      "1.335889458656311\n",
      "1.02734375\n",
      "0.9962484836578369\n",
      "1.6246323585510254\n",
      "1.597988486289978\n",
      "1.6635334491729736\n",
      "2.57802677154541\n",
      "2.016523838043213\n",
      "1.5835554599761963\n",
      "1.0226426124572754\n",
      "2.1017398834228516\n",
      "2.005934953689575\n",
      "1.906479001045227\n",
      "1.4081907272338867\n",
      "1.4420689344406128\n",
      "1.3558300733566284\n",
      "1.911588191986084\n",
      "1.210157871246338\n",
      "2.4431920051574707\n",
      "2.1948013305664062\n",
      "2.2849974632263184\n",
      "2.610792636871338\n",
      "1.8133714199066162\n",
      "2.3049213886260986\n",
      "2.3221113681793213\n",
      "2.457827568054199\n",
      "2.2631115913391113\n",
      "2.2480812072753906\n",
      "2.4038033485412598\n",
      "2.3148977756500244\n",
      "2.229520797729492\n",
      "2.256654739379883\n",
      "2.207669496536255\n",
      "2.098263740539551\n",
      "1.5157562494277954\n",
      "1.9091832637786865\n",
      "1.7102019786834717\n",
      "2.3036422729492188\n",
      "1.5805141925811768\n",
      "1.457560420036316\n",
      "1.904937505722046\n",
      "1.6048084497451782\n",
      "2.8150525093078613\n",
      "2.1851940155029297\n",
      "1.734528660774231\n",
      "2.062833547592163\n",
      "2.2167446613311768\n",
      "2.5343637466430664\n",
      "2.411458730697632\n",
      "2.1690406799316406\n",
      "2.128588914871216\n",
      "2.1434364318847656\n",
      "1.1091196537017822\n",
      "2.1637496948242188\n",
      "2.1756162643432617\n",
      "1.7665607929229736\n",
      "2.1064565181732178\n",
      "1.1762598752975464\n",
      "1.3875658512115479\n",
      "1.1442975997924805\n",
      "1.2124907970428467\n",
      "1.9479939937591553\n",
      "1.6245278120040894\n",
      "2.0350351333618164\n",
      "1.7020952701568604\n",
      "1.5355252027511597\n",
      "1.6414711475372314\n",
      "1.649693489074707\n",
      "1.5056204795837402\n",
      "1.9449706077575684\n",
      "1.9135944843292236\n",
      "1.913515567779541\n",
      "1.0976333618164062\n",
      "1.4294203519821167\n",
      "1.3894110918045044\n",
      "1.4778399467468262\n",
      "1.4942166805267334\n",
      "1.4972755908966064\n",
      "1.3472310304641724\n",
      "1.4565696716308594\n",
      "1.9450939893722534\n",
      "1.3673187494277954\n",
      "1.3944737911224365\n",
      "0.9960724115371704\n",
      "2.0005459785461426\n",
      "1.4483907222747803\n",
      "2.1015121936798096\n",
      "2.0948400497436523\n",
      "2.1818654537200928\n",
      "2.148501396179199\n",
      "2.107664108276367\n",
      "2.1262588500976562\n",
      "1.469200611114502\n",
      "0.8492907881736755\n",
      "0.9003651142120361\n",
      "1.7550184726715088\n",
      "1.551711916923523\n",
      "1.5845272541046143\n",
      "0.9780263304710388\n",
      "1.2820305824279785\n",
      "2.380551338195801\n",
      "1.7607169151306152\n",
      "2.332958936691284\n",
      "1.4904356002807617\n",
      "2.010730743408203\n",
      "1.5818393230438232\n",
      "2.3360702991485596\n",
      "1.5703871250152588\n",
      "2.1001110076904297\n",
      "1.268540620803833\n",
      "1.5523734092712402\n",
      "1.6137958765029907\n",
      "2.803616523742676\n",
      "1.6967144012451172\n",
      "2.042360305786133\n",
      "1.932665467262268\n",
      "1.4546728134155273\n",
      "1.6323151588439941\n",
      "1.6050232648849487\n",
      "2.165898561477661\n",
      "1.3026254177093506\n",
      "2.1598668098449707\n",
      "2.120731830596924\n",
      "1.9952763319015503\n",
      "2.1577234268188477\n",
      "2.021146297454834\n",
      "1.85026216506958\n",
      "2.093170642852783\n",
      "2.1879682540893555\n",
      "2.0312275886535645\n",
      "2.0354199409484863\n",
      "2.067168712615967\n",
      "2.2082760334014893\n",
      "2.2039315700531006\n",
      "1.8728489875793457\n",
      "1.9254499673843384\n",
      "1.8582797050476074\n",
      "1.8680423498153687\n",
      "2.0910604000091553\n",
      "1.909313678741455\n",
      "1.8841917514801025\n",
      "1.8420517444610596\n",
      "1.8698151111602783\n",
      "1.7751734256744385\n",
      "1.9482262134552002\n",
      "2.00065279006958\n",
      "2.522268533706665\n",
      "2.2306971549987793\n",
      "1.6145473718643188\n",
      "1.7663953304290771\n",
      "1.7952322959899902\n",
      "1.7658522129058838\n",
      "1.829628348350525\n",
      "1.5123188495635986\n",
      "1.933901309967041\n",
      "1.977644920349121\n",
      "1.9600650072097778\n",
      "2.0439043045043945\n",
      "2.0289082527160645\n",
      "1.8813846111297607\n",
      "1.899658441543579\n",
      "1.856689214706421\n",
      "1.9337234497070312\n",
      "1.8622846603393555\n",
      "1.822402834892273\n",
      "1.651320219039917\n",
      "1.635329008102417\n",
      "2.1807351112365723\n",
      "1.69287109375\n",
      "1.645535945892334\n",
      "1.6065897941589355\n",
      "1.649062156677246\n",
      "1.6307692527770996\n",
      "1.6545333862304688\n",
      "2.5816731452941895\n",
      "2.392512559890747\n",
      "2.7754454612731934\n",
      "1.6915900707244873\n",
      "3.082465171813965\n",
      "2.1816585063934326\n",
      "1.5598223209381104\n",
      "1.5991367101669312\n",
      "2.1247315406799316\n",
      "1.7961126565933228\n",
      "2.337031602859497\n",
      "1.7594821453094482\n",
      "2.799046039581299\n",
      "2.042682647705078\n",
      "1.8133809566497803\n",
      "2.4353315830230713\n",
      "1.7769076824188232\n",
      "2.309659957885742\n",
      "1.803133487701416\n",
      "2.215506076812744\n",
      "2.495760679244995\n",
      "2.1549458503723145\n",
      "2.3532896041870117\n",
      "2.4149837493896484\n",
      "2.1655287742614746\n",
      "2.1520049571990967\n",
      "2.130939245223999\n",
      "1.5195060968399048\n",
      "1.593585729598999\n",
      "1.642869234085083\n",
      "1.6102664470672607\n",
      "1.6694620847702026\n",
      "1.6462866067886353\n",
      "1.560239315032959\n",
      "1.60585355758667\n",
      "1.5100574493408203\n",
      "1.7008287906646729\n",
      "1.6656951904296875\n",
      "1.6371064186096191\n",
      "1.5238089561462402\n",
      "1.6533548831939697\n",
      "1.6558771133422852\n",
      "1.7100715637207031\n",
      "1.2994110584259033\n",
      "1.2777713537216187\n",
      "1.4285250902175903\n",
      "1.8344781398773193\n",
      "1.4231395721435547\n",
      "1.5815590620040894\n",
      "1.432011365890503\n",
      "1.5182156562805176\n",
      "1.732467532157898\n",
      "1.701855182647705\n",
      "1.5647544860839844\n",
      "1.460197925567627\n",
      "1.4193601608276367\n",
      "1.5724735260009766\n",
      "1.2076836824417114\n",
      "1.516430139541626\n",
      "1.5008878707885742\n",
      "1.215425729751587\n",
      "1.6230236291885376\n",
      "1.7173484563827515\n",
      "1.7976100444793701\n",
      "1.0905264616012573\n",
      "1.0632219314575195\n",
      "1.5550299882888794\n",
      "1.5081956386566162\n",
      "1.5231037139892578\n",
      "1.6149630546569824\n",
      "1.3937560319900513\n",
      "1.9441900253295898\n",
      "1.4105076789855957\n",
      "1.5431957244873047\n",
      "1.9128646850585938\n",
      "1.0203204154968262\n",
      "1.329235553741455\n",
      "0.9824995994567871\n",
      "1.340240478515625\n",
      "1.2983083724975586\n",
      "1.2916040420532227\n",
      "1.300654649734497\n",
      "1.2443894147872925\n",
      "1.3297467231750488\n",
      "1.3011749982833862\n",
      "1.2235081195831299\n",
      "1.660060167312622\n",
      "1.636967658996582\n",
      "1.6772708892822266\n",
      "0.9277613162994385\n",
      "0.8301488161087036\n",
      "1.2907261848449707\n",
      "1.1741302013397217\n",
      "1.1611210107803345\n",
      "1.1603937149047852\n",
      "1.4875351190567017\n",
      "1.130158543586731\n",
      "0.8755053281784058\n",
      "1.070465326309204\n",
      "1.13592529296875\n",
      "0.8328679203987122\n",
      "0.8540888428688049\n",
      "0.8727591633796692\n",
      "0.8977503776550293\n",
      "0.7192800045013428\n",
      "1.6703680753707886\n",
      "2.4842538833618164\n",
      "1.4833903312683105\n",
      "1.5609028339385986\n",
      "1.4898972511291504\n",
      "1.4231377840042114\n",
      "1.5767573118209839\n",
      "2.3691630363464355\n",
      "1.6613713502883911\n",
      "1.591747760772705\n",
      "2.1109628677368164\n",
      "0.9226978421211243\n",
      "0.6544594764709473\n",
      "1.474859356880188\n",
      "1.6497682332992554\n",
      "2.4473013877868652\n",
      "2.8779659271240234\n",
      "2.6362318992614746\n",
      "2.261159896850586\n",
      "0.5507780313491821\n",
      "0.549339771270752\n",
      "0.625969648361206\n",
      "2.144735336303711\n",
      "2.7359910011291504\n",
      "1.389082670211792\n",
      "1.4122660160064697\n",
      "1.9473471641540527\n",
      "1.384636402130127\n",
      "1.5233166217803955\n",
      "1.8805855512619019\n",
      "1.9282500743865967\n",
      "1.8595964908599854\n",
      "1.916464924812317\n",
      "1.9602015018463135\n",
      "2.1848275661468506\n",
      "2.1393842697143555\n",
      "1.8809901475906372\n",
      "2.0338149070739746\n",
      "1.138502836227417\n",
      "1.792614221572876\n",
      "1.6930649280548096\n",
      "1.6105681657791138\n",
      "1.6099352836608887\n",
      "0.7651305198669434\n",
      "1.9933104515075684\n",
      "1.8316564559936523\n",
      "1.9454574584960938\n",
      "1.9751665592193604\n",
      "1.6097966432571411\n",
      "1.9667028188705444\n",
      "0.936764121055603\n",
      "1.0304676294326782\n",
      "1.7476062774658203\n",
      "1.8685736656188965\n",
      "1.1448251008987427\n",
      "2.112450361251831\n",
      "2.161198139190674\n",
      "0.9329229593276978\n",
      "1.0214686393737793\n",
      "1.1629725694656372\n",
      "1.9126983880996704\n",
      "2.034287214279175\n",
      "1.9684932231903076\n",
      "3.2298696041107178\n",
      "1.9614132642745972\n",
      "2.3210508823394775\n",
      "2.2678098678588867\n",
      "2.9998490810394287\n",
      "1.8333550691604614\n",
      "2.6780645847320557\n",
      "2.5939223766326904\n",
      "3.05678129196167\n",
      "2.1565723419189453\n",
      "2.9972007274627686\n",
      "2.059548854827881\n",
      "2.6802687644958496\n",
      "2.1445059776306152\n",
      "2.3455452919006348\n",
      "1.3937633037567139\n",
      "2.193197011947632\n",
      "2.1767196655273438\n",
      "1.3204940557479858\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     17\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m:input_values,\n\u001b[1;32m     18\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m:batch[\u001b[39m'\u001b[39m\u001b[39mwav_mask\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     20\u001b[0m inputs \u001b[39m=\u001b[39m {key: inputs[key]\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m inputs}\n\u001b[0;32m---> 23\u001b[0m logits \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     26\u001b[0m \u001b[39m# outputs = torch.argmax(logits, dim=-1)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# print(logi)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits, labels\u001b[39m.\u001b[39mlong())\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1814\u001b[0m, in \u001b[0;36mWav2Vec2ForSequenceClassification.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1811\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1812\u001b[0m output_hidden_states \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_weighted_layer_sum \u001b[39melse\u001b[39;00m output_hidden_states\n\u001b[0;32m-> 1814\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwav2vec2(\n\u001b[1;32m   1815\u001b[0m     input_values,\n\u001b[1;32m   1816\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1817\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1818\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1819\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1820\u001b[0m )\n\u001b[1;32m   1822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_weighted_layer_sum:\n\u001b[1;32m   1823\u001b[0m     hidden_states \u001b[39m=\u001b[39m outputs[_HIDDEN_STATES_START_POSITION]\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1315\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1310\u001b[0m hidden_states, extract_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_projection(extract_features)\n\u001b[1;32m   1311\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mask_hidden_states(\n\u001b[1;32m   1312\u001b[0m     hidden_states, mask_time_indices\u001b[39m=\u001b[39mmask_time_indices, attention_mask\u001b[39m=\u001b[39mattention_mask\n\u001b[1;32m   1313\u001b[0m )\n\u001b[0;32m-> 1315\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1316\u001b[0m     hidden_states,\n\u001b[1;32m   1317\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1318\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1319\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1320\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1321\u001b[0m )\n\u001b[1;32m   1323\u001b[0m hidden_states \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:888\u001b[0m, in \u001b[0;36mWav2Vec2EncoderStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    882\u001b[0m         layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    883\u001b[0m             create_custom_forward(layer),\n\u001b[1;32m    884\u001b[0m             hidden_states,\n\u001b[1;32m    885\u001b[0m             attention_mask,\n\u001b[1;32m    886\u001b[0m         )\n\u001b[1;32m    887\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         layer_outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    889\u001b[0m             hidden_states, attention_mask\u001b[39m=\u001b[39;49mattention_mask, output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    890\u001b[0m         )\n\u001b[1;32m    891\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    893\u001b[0m \u001b[39mif\u001b[39;00m skip_the_layer:\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:720\u001b[0m, in \u001b[0;36mWav2Vec2EncoderLayerStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    718\u001b[0m attn_residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    719\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 720\u001b[0m hidden_states, attn_weights, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    721\u001b[0m     hidden_states, attention_mask\u001b[39m=\u001b[39;49mattention_mask, output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    723\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    724\u001b[0m hidden_states \u001b[39m=\u001b[39m attn_residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/etri-erc/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:620\u001b[0m, in \u001b[0;36mWav2Vec2Attention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    616\u001b[0m     attn_weights_reshaped \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    618\u001b[0m attn_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(attn_weights, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m--> 620\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(attn_probs, value_states)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m attn_output\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim):\n\u001b[1;32m    623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    624\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`attn_output` should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz,\u001b[39m \u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\u001b[39m \u001b[39mtgt_len,\u001b[39m \u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m}\u001b[39;00m\u001b[39m, but is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mattn_output\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    626\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size= 2)\n",
    "\n",
    "\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, training_dataloader = accelerator.prepare(\n",
    "     model, optimizer, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_loss = 0\n",
    "train_acc_sum = 0\n",
    "train_loss = []\n",
    "for step, batch in enumerate(train_loader): \n",
    "    optimizer.zero_grad()\n",
    "    labels = (batch['emotion']).to(device)\n",
    "    input_values = processor(batch[\"wav\"],\n",
    "                             sampling_rate=16000,\n",
    "                             return_tensors=\"pt\",\n",
    "                             return_attention_mask = False)['input_values'].squeeze()\n",
    "    inputs = {\"input_values\":input_values,\n",
    "              \"attention_mask\":batch['wav_mask'],\n",
    "    }\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "\n",
    "\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "    \n",
    "    # outputs = torch.argmax(logits, dim=-1)\n",
    "    # print(logi)\n",
    "\n",
    "    loss = criterion(logits, labels.long())\n",
    "    total_loss += loss.item()\n",
    "    train_loss.append(total_loss/(step+1))\n",
    "    # print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "avg_train_loss = total_loss / len(train_loader)\n",
    "print(f'  Average training loss: {avg_train_loss:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape)\n",
    "print(labels.shape)\n",
    "print(labels.dtype)\n",
    "print(logits.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = model.config.id2label[predicted_class_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EmotionClassfierWithAudio(nn.Module):\n",
    "    def __init__(self,pretrained_model, n_classes=7):\n",
    "        super().__init__()\n",
    "        self.pretrianed_model = pretrained_model\n",
    "        # for p in self.pretrianed_model.parameters():\n",
    "            # p.requires_grad = False\n",
    "\n",
    "        self.fc = nn.Linear(1025,n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.adaptive_avg_pool3d(self.pretrianed_model(x).logits , axis=1).squeeze()# mean of 355 // logit output shape: B, Seq(355), 1025\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jiwer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "# import soundfile as sf\n",
    "import torch\n",
    "from jiwer import wer # wer metircs \n",
    "from transformers import Wav2Vec2Processor\n",
    "batch = train_dataset[2]\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "inputs = processor(batch[\"wav\"], sampling_rate=16000, return_tensors=\"pt\", return_attention_mask = False,\n",
    "                #    padding=\"longest\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['attention_mask'].squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\").to('cuda')\n",
    "\n",
    "input_values = inputs.input_values\n",
    "print(input_values.shape)\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values.type(torch.FloatTensor).to(\"cuda\")).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(batch['txt'])\n",
    "print(transcription)\n",
    "print(\"WER:\", wer(batch['txt'], transcription))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(input_values.type(torch.FloatTensor).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etri-erc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d91dba4ef89c422bc4f5a90183fbfeb50a0fdb361fb291167f0137a8b87ec3ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
