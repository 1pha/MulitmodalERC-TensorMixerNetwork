{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate_datasets' from 'erc.preprocess' (/home/hoesungryu/etri-erc/erc/preprocess.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm \n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39merc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocess\u001b[39;00m \u001b[39mimport\u001b[39;00m generate_datasets\n\u001b[1;32m     10\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     11\u001b[0m logger\u001b[39m.\u001b[39msetLevel(logging\u001b[39m.\u001b[39mINFO)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'generate_datasets' from 'erc.preprocess' (/home/hoesungryu/etri-erc/erc/preprocess.py)"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import hydra\n",
    "\n",
    "from erc.preprocess import generate_datasets\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "with hydra.initialize(version_base=None, config_path=\"./config\"):\n",
    "    cfg = hydra.compose(config_name=\"config\", overrides={\"dataset._target_=erc.datasets.KEMDy19Dataset\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change torch dataset into huggingface dataset ... \n",
    "fold_num = 1\n",
    "train_dataset = hydra.utils.instantiate(cfg.dataset, mode = \"train\", validation_fold = fold_num)\n",
    "valid_dataset = hydra.utils.instantiate(cfg.dataset, mode = \"valid\", validation_fold = fold_num)\n",
    "\n",
    "train_ds = generate_datasets(\n",
    "    train_dataset,\n",
    "    save_name = 'audio_dataset_19',\n",
    "    mode  = 'train',\n",
    "    validation_fold =fold_num,\n",
    "    overrides=False\n",
    ")\n",
    "valid_ds = generate_datasets(\n",
    "    valid_dataset,\n",
    "    save_name = 'audio_dataset_19',\n",
    "    mode  = 'valid',\n",
    "    validation_fold =fold_num,\n",
    "    overrides=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wav2Vec2 \n",
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "from erc.constants import idx2emotion, emotion2idx\n",
    "\n",
    "\n",
    "# default value \n",
    "model_name_or_path = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
    "num_labels = 7 \n",
    "pooling_mode = \"mean\" # max or min \n",
    "\n",
    "\n",
    "# set config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    label2id=emotion2idx,\n",
    "    id2label=idx2emotion,\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(f\"The target sampling rate: {target_sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "from jiwer import wer # wer metircs \n",
    "from transformers import Wav2Vec2Processor\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-training Scheme ... \n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "\n",
    "# pretrain_str = \"w11wo/wav2vec2-xls-r-300m-korean\"\n",
    "\n",
    "processor= Wav2Vec2Processor.from_pretrained(pretrain_str)\n",
    "pretrained_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    # \"wav2vec2-xls-r-300m-korean\",\n",
    "    pretrain_str,\n",
    "    num_labels=7\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import erc\n",
    "\n",
    "erc.utils.count_parameters(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pretrained_model.to(device)\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5,  eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size= 2)\n",
    "\n",
    "\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, training_dataloader = accelerator.prepare(\n",
    "     model, optimizer, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_loss = 0\n",
    "train_acc_sum = 0\n",
    "train_loss = []\n",
    "for step, batch in enumerate(train_loader): \n",
    "    optimizer.zero_grad()\n",
    "    labels = (batch['emotion']).to(device)\n",
    "    input_values = processor(batch[\"wav\"],\n",
    "                             sampling_rate=16000,\n",
    "                             return_tensors=\"pt\",\n",
    "                             return_attention_mask = False)['input_values'].squeeze()\n",
    "    inputs = {\"input_values\":input_values,\n",
    "              \"attention_mask\":batch['wav_mask'],\n",
    "    }\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "\n",
    "\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "    \n",
    "    # outputs = torch.argmax(logits, dim=-1)\n",
    "    # print(logi)\n",
    "\n",
    "    loss = criterion(logits, labels.long())\n",
    "    total_loss += loss.item()\n",
    "    train_loss.append(total_loss/(step+1))\n",
    "    # print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "avg_train_loss = total_loss / len(train_loader)\n",
    "print(f'  Average training loss: {avg_train_loss:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape)\n",
    "print(labels.shape)\n",
    "print(labels.dtype)\n",
    "print(logits.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = model.config.id2label[predicted_class_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EmotionClassfierWithAudio(nn.Module):\n",
    "    def __init__(self,pretrained_model, n_classes=7):\n",
    "        super().__init__()\n",
    "        self.pretrianed_model = pretrained_model\n",
    "        # for p in self.pretrianed_model.parameters():\n",
    "            # p.requires_grad = False\n",
    "\n",
    "        self.fc = nn.Linear(1025,n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.adaptive_avg_pool3d(self.pretrianed_model(x).logits , axis=1).squeeze()# mean of 355 // logit output shape: B, Seq(355), 1025\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jiwer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "# import soundfile as sf\n",
    "import torch\n",
    "from jiwer import wer # wer metircs \n",
    "from transformers import Wav2Vec2Processor\n",
    "batch = train_dataset[2]\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "inputs = processor(batch[\"wav\"], sampling_rate=16000, return_tensors=\"pt\", return_attention_mask = False,\n",
    "                #    padding=\"longest\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['attention_mask'].squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\").to('cuda')\n",
    "\n",
    "input_values = inputs.input_values\n",
    "print(input_values.shape)\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values.type(torch.FloatTensor).to(\"cuda\")).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(batch['txt'])\n",
    "print(transcription)\n",
    "print(\"WER:\", wer(batch['txt'], transcription))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(input_values.type(torch.FloatTensor).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d91dba4ef89c422bc4f5a90183fbfeb50a0fdb361fb291167f0137a8b87ec3ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
