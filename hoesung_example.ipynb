{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import hydra\n",
    "\n",
    "import erc\n",
    "\n",
    "logger = erc.utils.get_logger()\n",
    "\n",
    "with hydra.initialize(version_base=None, config_path=\"./config\"):\n",
    "    cfg = hydra.compose(config_name=\"config\", overrides={\"dataset._target_=erc.datasets.KEMDy19Dataset\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:erc.utils:Instantiate KEMDy19 Dataset\n",
      "INFO:erc.utils:Instantiate KEMDy19 Dataset\n"
     ]
    }
   ],
   "source": [
    "# change torch dataset into huggingface dataset ... \n",
    "fold_num = 1\n",
    "train_dataset = hydra.utils.instantiate(cfg.dataset, mode = \"train\", validation_fold = fold_num)\n",
    "valid_dataset = hydra.utils.instantiate(cfg.dataset, mode = \"valid\", validation_fold = fold_num)\n",
    "\n",
    "train_ds = erc.preprocess.generate_datasets(\n",
    "    train_dataset,\n",
    "    save_name = 'audio_dataset_19',\n",
    "    mode  = 'train',\n",
    "    validation_fold =fold_num,\n",
    "    overrides=False\n",
    ")\n",
    "valid_ds = erc.preprocess.generate_datasets(\n",
    "    valid_dataset,\n",
    "    save_name = 'audio_dataset_19',\n",
    "    mode  = 'valid',\n",
    "    validation_fold =fold_num,\n",
    "    overrides=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/hoesungryu/etri-erc/audio_dataset_19/train_01/cache-06b15f15dca37a05.arrow\n"
     ]
    }
   ],
   "source": [
    "# apply tokenizer to the txt \n",
    "def map_to_str_with_tokenizer(batch,\n",
    "                              tokenizer):\n",
    "\n",
    "    batch['txt'] = tokenizer(batch['txt'])\n",
    "\n",
    "    return batch\n",
    "# test_ds = test_ds.map(map_to_str_with_tokenizer)\n",
    "\n",
    "# apply tokenizer to the txt \n",
    "def map_labels2long(batch,):\n",
    "\n",
    "    batch['labels'] = batch['labels'].long()\n",
    "\n",
    "    return batch\n",
    "train_ds = train_ds.map(map_labels2long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/hoesungryu/etri-erc/audio_dataset_19/valid_01/cache-1e44356524176d77.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_ds = valid_ds.map(map_labels2long)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wav2Vec2 \n",
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "from erc.constants import idx2emotion, emotion2idx\n",
    "\n",
    "\n",
    "# default value \n",
    "model_name_or_path = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
    "num_labels = 7 \n",
    "pooling_mode = \"mean\" # max or min \n",
    "\n",
    "# set config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    label2id=emotion2idx,\n",
    "    id2label=idx2emotion,\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    "    pooling_mode = 'mean'\n",
    ")\n",
    "\n",
    "# setattr(config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kresnik/wav2vec2-large-xlsr-korean were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at kresnik/wav2vec2-large-xlsr-korean and are newly initialized: ['projector.weight', 'classifier.bias', 'projector.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification\n",
    "\n",
    "pretrained_model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name_or_path, config=config)\n",
    "processor= Wav2Vec2Processor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    is_apex_available,\n",
    ")\n",
    "\n",
    "# if is_apex_available():\n",
    "    # from apex import amp\n",
    "\n",
    "# if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    # _is_native_amp_available = True\n",
    "\n",
    "from torch.cuda import amp\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # if self.use_amp:\n",
    "        # with autocast():\n",
    "            # loss = self.compute_loss(model, inputs)\n",
    "        # else:\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        # if self.use_amp:\n",
    "        # self.scaler.scale(loss).backward()\n",
    "        # elif self.use_apex:\n",
    "        # with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "            # scaled_loss.backward()\n",
    "        # elif self.deepspeed:\n",
    "            # self.deepspeed.backward(loss)\n",
    "        # else:\n",
    "        loss.mean().backward()\n",
    "\n",
    "        return loss.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction, is_regression = False):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "    \n",
    "    if is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-xlsr-speech-emotion-classification\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1.0,\n",
    "    fp16=True,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = CTCTrainer(\n",
    "#     model=pretrained_model,\n",
    "#     # data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     # compute_metrics=compute_metrics,\n",
    "#     train_dataset=train_ds,\n",
    "#     eval_dataset=valid_ds,\n",
    "#     tokenizer= processor.feature_extractor,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=pretrained_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer= processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id. If id are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/hoesungryu/.conda/envs/erc/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7525\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 314\n",
      "  Number of trainable parameters = 315702919\n",
      "/home/hoesungryu/.conda/envs/erc/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/314 00:01 < 08:14, 0.63 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id. If id are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1835\n",
      "  Batch size = 12\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1835\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-10\n",
      "Configuration saved in ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-10/config.json\n",
      "Model weights saved in ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-10/pytorch_model.bin\n",
      "Feature extractor saved in ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-10/preprocessor_config.json\n",
      "/home/hoesungryu/.conda/envs/erc/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id. If id are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1835\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-20\n",
      "Configuration saved in ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-20/config.json\n",
      "Model weights saved in ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-20/pytorch_model.bin\n",
      "Feature extractor saved in ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-20/preprocessor_config.json\n",
      "/home/hoesungryu/.conda/envs/erc/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: id. If id are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1835\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-30\n",
      "Configuration saved in ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-30/config.json\n",
      "Model weights saved in ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-30/pytorch_model.bin\n",
      "Feature extractor saved in ./wav2vec2-xlsr-speech-emotion-classification/checkpoint-30/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-xlsr-speech-emotion-classification/checkpoint-10] due to args.save_total_limit\n",
      "/home/hoesungryu/.conda/envs/erc/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "from jiwer import wer # wer metircs \n",
    "from transformers import Wav2Vec2Processor\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-training Scheme ... \n",
    "\n",
    "\n",
    "\n",
    "# pretrain_str = \"w11wo/wav2vec2-xls-r-300m-korean\"\n",
    "\n",
    "\n",
    "pretrained_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    # \"wav2vec2-xls-r-300m-korean\",\n",
    "    pretrain_str,\n",
    "    num_labels=7\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import erc\n",
    "\n",
    "erc.utils.count_parameters(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pretrained_model.to(device)\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5,  eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_loss = 0\n",
    "train_acc_sum = 0\n",
    "train_loss = []\n",
    "for step, batch in enumerate(train_loader): \n",
    "    optimizer.zero_grad()\n",
    "    labels = (batch['emotion']).to(device)\n",
    "    input_values = processor(batch[\"wav\"],\n",
    "                             sampling_rate=16000,\n",
    "                             return_tensors=\"pt\",\n",
    "                             return_attention_mask = False)['input_values'].squeeze()\n",
    "    inputs = {\"input_values\":input_values,\n",
    "              \"attention_mask\":batch['wav_mask'],\n",
    "    }\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "\n",
    "\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "    \n",
    "    # outputs = torch.argmax(logits, dim=-1)\n",
    "    # print(logi)\n",
    "\n",
    "    loss = criterion(logits, labels.long())\n",
    "    total_loss += loss.item()\n",
    "    train_loss.append(total_loss/(step+1))\n",
    "    # print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "avg_train_loss = total_loss / len(train_loader)\n",
    "print(f'  Average training loss: {avg_train_loss:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape)\n",
    "print(labels.shape)\n",
    "print(labels.dtype)\n",
    "print(logits.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = model.config.id2label[predicted_class_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EmotionClassfierWithAudio(nn.Module):\n",
    "    def __init__(self,pretrained_model, n_classes=7):\n",
    "        super().__init__()\n",
    "        self.pretrianed_model = pretrained_model\n",
    "        # for p in self.pretrianed_model.parameters():\n",
    "            # p.requires_grad = False\n",
    "\n",
    "        self.fc = nn.Linear(1025,n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.adaptive_avg_pool3d(self.pretrianed_model(x).logits , axis=1).squeeze()# mean of 355 // logit output shape: B, Seq(355), 1025\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jiwer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "# import soundfile as sf\n",
    "import torch\n",
    "from jiwer import wer # wer metircs \n",
    "from transformers import Wav2Vec2Processor\n",
    "batch = train_dataset[2]\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "inputs = processor(batch[\"wav\"], sampling_rate=16000, return_tensors=\"pt\", return_attention_mask = False,\n",
    "                #    padding=\"longest\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['wav_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['attention_mask'].squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\").to('cuda')\n",
    "\n",
    "input_values = inputs.input_values\n",
    "print(input_values.shape)\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values.type(torch.FloatTensor).to(\"cuda\")).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(batch['txt'])\n",
    "print(transcription)\n",
    "print(\"WER:\", wer(batch['txt'], transcription))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(input_values.type(torch.FloatTensor).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d91dba4ef89c422bc4f5a90183fbfeb50a0fdb361fb291167f0137a8b87ec3ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
