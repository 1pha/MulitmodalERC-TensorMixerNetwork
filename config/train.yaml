hydra:
  job:
    chdir: False  # keep hydra = 1.1 change directory behavior
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  
defaults:
  - _self_
  - dataset: hf
  - model: combined

dataloader:
  _target_: torch.utils.data.DataLoader
  batch_size: 12
  num_workers: 4
  dataset: ${dataset}

optim:
  _target_: torch.optim.AdamW
  eps: 1e-8
  lr: 1e-5
  weight_decay: 1e-6

# Not in hydra style, but rather use `configure_optimizers` style
# https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule.configure_optimizers
# 
sch:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  T_0: 500
  T_mult: 2
  eta_min: 1e-7

# sch_config:
#   interval: step
#   monitor:
#   frequency:
#   strict: False
#   name: learning rate

misc:
  seed: 42
  debug: False
  # "w11wo/wav2vec2-xls-r-300m-korean" for other pre-trained

module:
  _target_: erc.trainer.ERCModule

logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  project: etri-erc
  entity: etri-erc
  name: ERC-wav

# https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 200
  devices: 1
  accelerator: gpu
  gradient_clip_val: 1
  log_every_n_steps: 100
  # DEBUGGING FLAGS. TODO: Split
  # limit_train_batches: 0.001
  # limit_val_batches: 0.01

callbacks:
  # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.ModelCheckpoint.html
  checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: ${hydra:run.dir}
    filename: "{step}-valid_loss{epoch/valid_loss:.3f}"
    monitor: "epoch/valid_loss"
    mode: min
    save_top_k: 3
    save_last: True
    # Is useful to set it to False when metric names contain / as this will result in extra folders
    auto_insert_metric_name: False

  # https://pytorch-lightning.readthedocs.io/en/stable/common/early_stopping.html
  early_stop:
    _target_: pytorch_lightning.callbacks.early_stopping.EarlyStopping
    monitor: "epoch/valid_loss"
    mode: min
    patience: 3
  
  # https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.LearningRateMonitor.html
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
    log_momentum: False